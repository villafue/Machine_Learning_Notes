{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 Creating Features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN5vMHcIBeiOGQMITtmtwQn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Feature%20Engineering/Feature%20Engineering%20for%20Machine%20Learning%20in%20Python/1%20Creating%20Features/1_Creating_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkCgIOtCZaWo"
      },
      "source": [
        "# Creating Features\r\n",
        "\r\n",
        "In this chapter, you will explore what feature engineering is and how to get started with applying it to real-world data. You will load, explore and visualize a survey response dataset, and in doing so you will learn about its underlying data types and why they have an influence on how you should engineer your features. Using the pandas package you will create new features from both categorical and continuous columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcW0lP95Zflu"
      },
      "source": [
        "# 1. Why generate features?\r\n",
        "\r\n",
        "Hello and welcome to Feature Engineering for Machine Learning in Python. My name is Robert O’Callaghan and I am a Data Scientist.\r\n",
        "\r\n",
        "2. Feature Engineering\r\n",
        "Feature engineering is the act of taking raw data and extracting features from it that are suitable for tasks like machine learning. Most machine learning algorithms work with tabular data. When we talk about features, we are referring to the information stored in the columns of these tables. For example, if we were looking at information on houses, the features would be things like square foot, number of rooms, etc. This course is designed for data scientists who want to expand their knowledge of how to incorporate feature engineering into their data science workflow.\r\n",
        "\r\n",
        "3. Different types of data\r\n",
        "Most machine learning algorithms require their input data to be represented as a vector or a matrix, and many assume that the data is distributed normally. In the real world, more often than not you will receive data that is not in this format. You will also need to work with many different types of data, some data types you will often encounter are: continuous variables, categorical data, ordinal data, boolean values, and dates and times. Dealing with these is manageable, but requires a well thought out approach. Feature engineering is often overlooked in machine learning discussions, but any real-world practitioner will confirm that data manipulation and feature engineering is the most important aspect of the project.\r\n",
        "\r\n",
        "4. Course structure\r\n",
        "Over the span of this course, we will be addressing how to deal with many different types of data and how to convert them into a format that can be easily used for machine learning. In the first chapter, you will ingest and create basic features from tabular data. In the second chapter, you will learn how to deal with data that has missing values. You will then move on to transforming your data so that it conforms to statistical assumptions often necessary for machine learning models, and finally, you will convert free form text into tabular data so it can be used with machine learning models.\r\n",
        "\r\n",
        "5. Pandas\r\n",
        "Now lets jump straight in with some examples. During this course we will be leveraging the pandas package substantially as it is very useful when working with data in tabular form. It is a common practice to import pandas using the pd alias. You can use the read_csv() function to import a CSV file and use the head() method to quickly look at the first few rows of the DataFrame.\r\n",
        "\r\n",
        "6. Dataset\r\n",
        "For the first three chapters of this course, you will be working with a modified subset of the Stackoverflow survey response data. This data set records the details and preferences of hundreds of users of the StackOverflow website.\r\n",
        "\r\n",
        "7. Column names\r\n",
        "To see the features used in this subset, you can use the DataFrame columns attribute to print the names of all the columns in the DataFrame.\r\n",
        "\r\n",
        "8. Column types\r\n",
        "To print the data type of each column, you can use the dtypes attribute. Here you can see three different data types - integers, floats and objects - in pandas objects are columns that contain strings.\r\n",
        "\r\n",
        "9. Selecting specific data types\r\n",
        "Knowing the types of each column can be very useful if you are performing analysis based on a subset of specific data types. To do this, you can use the select_dtypes() method and pass a list of relevant data types to the include argument. For example, if you want to select only the integer columns, call the select_dtypes() method on df and set the include argument to 'int'.\r\n",
        "\r\n",
        "10. Lets get going!\r\n",
        "Lets get right into it and start practicing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTRgxcgxc1dE"
      },
      "source": [
        "# Getting to know your data\r\n",
        "\r\n",
        "Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using read_csv().\r\n",
        "\r\n",
        "You will be working with a modified subset of the Stackoverflow survey response data in the first three chapters of this course. This data set records the details, and preferences of thousands of users of the StackOverflow website.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import the pandas library as pd.\r\n",
        "\r\n",
        "2. so_survey_csv contains the URL to a CSV file. Import it using Pandas into so_survey_df.\r\n",
        "\r\n",
        "3. Print the first five rows of so_survey_df.\r\n",
        "\r\n",
        "4. Print the data type of each column in so_survey_df.\r\n",
        "\r\n",
        "Question\r\n",
        "\r\n",
        "What type of data is the ConvertedSalary column?\r\n",
        "\r\n",
        "Wrong Answers\r\n",
        "\r\n",
        " - Datetime\r\n",
        "\r\n",
        " - String\r\n",
        "\r\n",
        " - Boolean\r\n",
        "\r\n",
        "Correct Answer\r\n",
        "\r\n",
        "Numeric - Correct! ConvertedSalary contains floats which are numeric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBWeOUp5X-YW"
      },
      "source": [
        "# Import pandas\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "# Import so_survey_csv into so_survey_df\r\n",
        "so_survey_df = pd.read_csv(so_survey_csv)\r\n",
        "\r\n",
        "# Print the first five rows of the DataFrame\r\n",
        "print(so_survey_df.head())\r\n",
        "\r\n",
        "# Print the data type of each column\r\n",
        "print(so_survey_df.dtypes)\r\n",
        "\r\n",
        "'''\r\n",
        "  SurveyDate                                    FormalEducation  ConvertedSalary Hobby       Country  ...     VersionControl Age  Years Experience  Gender   RawSalary\r\n",
        "0  2/28/18 20:20           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN   Yes  South Africa  ...                Git  21                13    Male         NaN\r\n",
        "1  6/28/18 13:26           Bachelor's degree (BA. BS. B.Eng.. etc.)          70841.0   Yes       Sweeden  ...     Git;Subversion  38                 9    Male   70,841.00\r\n",
        "2    6/6/18 3:37           Bachelor's degree (BA. BS. B.Eng.. etc.)              NaN    No       Sweeden  ...                Git  45                11     NaN         NaN\r\n",
        "3    5/9/18 1:06  Some college/university study without earning ...          21426.0   Yes       Sweeden  ...  Zip file back-ups  46                12    Male   21,426.00\r\n",
        "4  4/12/18 22:41           Bachelor's degree (BA. BS. B.Eng.. etc.)          41671.0   Yes            UK  ...                Git  39                 7    Male  £41,671.00\r\n",
        "\r\n",
        "[5 rows x 11 columns]\r\n",
        "SurveyDate                     object\r\n",
        "FormalEducation                object\r\n",
        "ConvertedSalary               float64\r\n",
        "Hobby                          object\r\n",
        "Country                        object\r\n",
        "StackOverflowJobsRecommend    float64\r\n",
        "VersionControl                 object\r\n",
        "Age                             int64\r\n",
        "Years Experience                int64\r\n",
        "Gender                         object\r\n",
        "RawSalary                      object\r\n",
        "dtype: object\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyK-VH1DfLbn"
      },
      "source": [
        "# Selecting specific data types\r\n",
        "\r\n",
        "Often a data set will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame.\r\n",
        "\r\n",
        "The DataFrame (so_survey_df) from the previous exercise is available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a subset of so_survey_df consisting of only the numeric (int and float) columns.\r\n",
        "\r\n",
        "2. Print the column names contained in so_survey_df_num."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIwIIrDVf0wM"
      },
      "source": [
        "# Create subset of only the numeric columns\r\n",
        "so_numeric_df = so_survey_df.select_dtypes(include=['int','float'])\r\n",
        "\r\n",
        "# Print the column names contained in so_survey_df_num\r\n",
        "print(so_numeric_df.columns)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Index(['ConvertedSalary', 'StackOverflowJobsRecommend', 'Age', 'Years Experience'], dtype='object')\r\n",
        "'''\r\n",
        "'''\r\n",
        "Conclusion:\r\n",
        "Well done! In the next lesson, you will learn the most common ways of dealing with categorical data.\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyyrM6rimGP"
      },
      "source": [
        "# 1. Dealing with Categorical Variables\r\n",
        "\r\n",
        "Categorical variables are used to represent groups that are qualitative in nature. Some examples are colors, such as blue, red, black etc. or country of birth, such as Ireland, England or USA. While these can easily be understood by a human, you will need to encode categorical features as numeric values to use them in your machine learning models.\r\n",
        "\r\n",
        "2. Encoding categorical features\r\n",
        "As an example, here is a table which consists of the country of residence of different respondents in the Stackoverflow survey. To get from qualitative inputs to quantitative features, one may naively think that assigning every category in a column a number would suffice, for example India could be 1, USA 2 etc. But these categories are unordered, so assigning this order may greatly penalize the effectiveness of your model. Thus, you cannot allocate arbitrary numbers to each category as that would imply some form of ordering in the categories.\r\n",
        "\r\n",
        "3. Encoding categorical features\r\n",
        "Instead, values can be encoded by creating additional binary features corresponding to whether each value was picked or not as shown in the table on the right. In doing so your model can leverage the information of what country is given, without inferring any order between the different options.\r\n",
        "\r\n",
        "4. Encoding categorical features\r\n",
        "There are two main approaches when representing categorical columns in this way, one hot encoding and dummy encoding. These are very similar and often confused. In fact, by default, pandas performs one-hot encoding when you use the get_dummies() function.\r\n",
        "\r\n",
        "5. One-hot encoding\r\n",
        "One-hot encoding converts n categories into n features as shown here. You can use the get_dummies() function to one-hot encode columns. The function takes a DataFrame and a list of categorical columns you want converted into one hot encoded columns, and returns an updated DataFrame with these columns included. Specifying a prefix with the prefix argument can improve readability like the letter C for country has been used here.\r\n",
        "\r\n",
        "6. Dummy encoding\r\n",
        "On the other hand, dummy encoding creates n-1 features for n categories, omitting the first category. Notice that this time there is no feature for France, the first category. In dummy encoding, the base value, France in this case, is encoded by the absence of all other countries as you can see on the last row here and its value is represented by the intercept. For dummy encoding, you can use the same get_dummies() function with an additional argument, drop_first set to True as shown here.\r\n",
        "\r\n",
        "7. One-hot vs. dummies\r\n",
        "Both these methods have different advantages. One-hot encoding generally creates much more explainable features, as each country will have its own weight that can be observed after training. But one must be aware that one hot encoding may create features that are entirely collinear due to the same information being represented multiple times.\r\n",
        "\r\n",
        "8. One-hot vs. dummies\r\n",
        "Take for example a simpler categorical column recording the sex of the survey takers. By recording a 1 for male the information of whether the person is female is already known when the male column is 0. This double representation can lead to instability in your models and dummy values would be more appropriate.\r\n",
        "\r\n",
        "9. Limiting your columns\r\n",
        "However, both one-hot encoding and dummy encoding may result in a huge number of columns being created if there are too many different categories in a column. In these cases, you may want to only create columns for the most common values. You can check the number of occurrences of different features in a column using the value_counts() method on a specific column.\r\n",
        "\r\n",
        "10. Limiting your columns\r\n",
        "Once you have your counts of occurrences, you can use it to limit what values you will include by first creating a mask of the values that occur less than n times. A mask is a list of booleans outlining which values in a column should be affected. First we find the categories that occur less than n times using the index attribute and wrap this inside the isin() method. After you create the mask, you can use it to replace these categories that occur less than n times with a value of your choice as shown here.\r\n",
        "\r\n",
        "11. Now you deal with categorical variables\r\n",
        "Lets put what has been learned into practice and work with some categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya0mwgfii6xc"
      },
      "source": [
        "# One-hot encoding and dummy variables\r\n",
        "\r\n",
        "To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as so_survey_df and focusing on its Country column.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "- One-hot encode the Country column, adding \"OH\" as a prefix for each column.\r\n",
        "\r\n",
        "- Create dummy variables for the Country column, adding \"DM\" as a prefix for each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74h02Fe0jaNL"
      },
      "source": [
        "# Convert the Country column to a one hot encoded Data Frame\r\n",
        "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\r\n",
        "\r\n",
        "# Print the columns names\r\n",
        "print(one_hot_encoded.columns)\r\n",
        "\r\n",
        "'''\r\n",
        "Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend',\r\n",
        "       'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'OH_France', 'OH_India',\r\n",
        "       'OH_Ireland', 'OH_Russia', 'OH_South Africa', 'OH_Spain', 'OH_Sweeden', 'OH_UK', 'OH_USA', \r\n",
        "       'OH_Ukraine'], dtype='object')\r\n",
        "'''\r\n",
        "\r\n",
        "# Create dummy variables for the Country column\r\n",
        "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\r\n",
        "\r\n",
        "# Print the columns names\r\n",
        "print(dummy.columns)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Index(['SurveyDate', 'FormalEducation', 'ConvertedSalary', 'Hobby', 'StackOverflowJobsRecommend',\r\n",
        "           'VersionControl', 'Age', 'Years Experience', 'Gender', 'RawSalary', 'DM_India', 'DM_Ireland',\r\n",
        "           'DM_Russia', 'DM_South Africa', 'DM_Spain', 'DM_Sweeden', 'DM_UK', 'DM_USA', 'DM_Ukraine'],\r\n",
        "          dtype='object')\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs7qXK-vkM7y"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great job! Did you notice that the column for France was missing when you created dummy variables? Now you can choose to use one-hot encoding or dummy variables where appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4GYHQZkNf5"
      },
      "source": [
        "# Dealing with uncommon categories\r\n",
        "\r\n",
        "Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science's favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences.\r\n",
        "\r\n",
        "Instructions \r\n",
        "\r\n",
        "- Extract the Country column of so_survey_df as a series and assign it to countries.\r\n",
        "\r\n",
        "- Find the counts of each category in the newly created countries series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlZPa-YpliVc"
      },
      "source": [
        "# Create a series out of the Country column\r\n",
        "countries = so_survey_df['Country']\r\n",
        "\r\n",
        "# Get the counts of each category\r\n",
        "country_counts = countries.value_counts()\r\n",
        "\r\n",
        "# Create a mask for only categories that occur less than 10 times\r\n",
        "mask = countries.isin(country_counts[country_counts < 10].index)\r\n",
        "\r\n",
        "# Label all other categories as Other\r\n",
        "countries[mask] = 'Other'\r\n",
        "\r\n",
        "# Print the updated category counts\r\n",
        "print(countries.value_counts())\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    South Africa    166\r\n",
        "    USA             164\r\n",
        "    Spain           134\r\n",
        "    Sweeden         119\r\n",
        "    France          115\r\n",
        "    Russia           97\r\n",
        "    India            95\r\n",
        "    UK               95\r\n",
        "    Other            14\r\n",
        "    Name: Country, dtype: int64\r\n",
        "'''\r\n",
        "'''\r\n",
        "Conclusion\r\n",
        "Good work, now you can work with large data sets while grouping low frequency categories.\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZh81ikOl344"
      },
      "source": [
        "# 1. Numeric variables\r\n",
        "As mentioned in the previous lesson, most machine learning models will require your data to be in numeric format. However, even if your raw data is all numeric, there is still a lot you can do to improve your features.\r\n",
        "\r\n",
        "2. Types of numeric features\r\n",
        "Numeric features can be used to represent a huge array of different characteristics and measurements. Pretty much anything that can be quantitatively measured can be recorded as numeric data. For example, age, the price of an item, counts, and even spatial data such as coordinates. Depending on the use case, numeric features can be treated in several different ways. We will work through a few of the considerations and possible feature engineering steps to keep in mind when dealing with numeric data.\r\n",
        "\r\n",
        "3. Does size matter?\r\n",
        "One of the first questions you should ask when working with numeric features is whether the magnitude of the feature is its most important trait, or just its direction. For example, if you had a dataset of restaurant health and safety ratings containing the number of times a restaurant had major violations, you might care far more about whether the restaurant had any major violations at all (as you would rather not take any chances), over whether it was a repeat offender. Looking at this toy dataset containing restaurant IDs and the number of times they had major violations, we can see that some restaurants have no major violations but many have one or more. We will be creating a new binary column representing whether or not a restaurant committed any violation.\r\n",
        "\r\n",
        "4. Binarizing numeric variables\r\n",
        "Here we first create a new column Binary_Violation and set it to zero. Then, we use the dot loc notation to find all rows where Number_of_Violations is greater than zero and set the Binary_Violation column to 1.\r\n",
        "\r\n",
        "5. Binarizing numeric variables\r\n",
        "As you can see here, all rows where Number_of_Violations is equal to 0 are also zeros in Binary_Violation. However, for all rows where Number_of_Violations is greater than zero is 1 in Binary_Violation.\r\n",
        "\r\n",
        "6. Binning numeric variables\r\n",
        "An extension of this is perhaps you wish to group a numeric variable into more than two bins. This is often useful for variables such as age, wage brackets, etc where exact numbers are less relevant than the general magnitude of the value. Consider the same dataset of restaurant health and safety ratings containing the number of times a restaurant has had major violations. This time we will be creating three groups, Group 1, for restaurants with no offenses, Group 2 for restaurants with one or two offenses and group 3 for all restaurants with three or more offenses. Bins are created by using the pandas' cut() function. You can define the intervals using the bins argument as shown here, which in this case is a list of 4 values. You can also pass a list of labels like so.\r\n",
        "\r\n",
        "7. Binning numeric variables\r\n",
        "Note as we want to include 0 in the first bin, we must set the leftmost edge to lower than that, so all values between negative infinity and 0 are labeled as 1, all values equal to 1 or 2 are labeled as 2, and values greater than 2 are labeled as 3.\r\n",
        "\r\n",
        "8. Lets start practicing!\r\n",
        "Now you know how to binarize and bin numeric columns, it's time for you to put this into practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-FvICLdx8Y3"
      },
      "source": [
        "# Binarizing columns\r\n",
        "\r\n",
        "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the so_survey_df data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled Paid_Job indicating whether each person is paid (their salary is greater than zero).\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a new column called Paid_Job filled with zeros.\r\n",
        "\r\n",
        "2. Replace all the Paid_Job values with a 1 where the corresponding ConvertedSalary is greater than 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ_-b1NnyjqO"
      },
      "source": [
        "# Create the Paid_Job column filled with zeros\r\n",
        "so_survey_df['Paid_Job'] = 0\r\n",
        "\r\n",
        "# Replace all the Paid_Job values where ConvertedSalary is > 0\r\n",
        "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\r\n",
        "\r\n",
        "# Print the first five rows of the columns\r\n",
        "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())\r\n",
        "\r\n",
        "'''\r\n",
        "   Paid_Job  ConvertedSalary\r\n",
        "0         0              0.0\r\n",
        "1         1          70841.0\r\n",
        "2         0              0.0\r\n",
        "3         1          21426.0\r\n",
        "4         1          41671.0\r\n",
        "'''\r\n",
        "'''\r\n",
        "Conclusion: Good work, binarizing columns can also be useful for your target variables.\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_v4yY0Dy_qW"
      },
      "source": [
        "# Binning values\r\n",
        "\r\n",
        "For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\r\n",
        "\r\n",
        "Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Bin the value of the ConvertedSalary column in so_survey_df into 5 equal bins, in a new column called equal_binned.\r\n",
        "\r\n",
        "2. Bin the ConvertedSalary column using the boundaries in the list bins and label the bins using labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55B806FJG928"
      },
      "source": [
        "# Bin the continuous variable ConvertedSalary into 5 bins\r\n",
        "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 5)\r\n",
        "\r\n",
        "# Print the first 5 rows of the equal_binned column\r\n",
        "print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())\r\n",
        "\r\n",
        "'''\r\n",
        "          equal_binned  ConvertedSalary\r\n",
        "0  (-2000.0, 400000.0]              0.0\r\n",
        "1  (-2000.0, 400000.0]          70841.0\r\n",
        "2  (-2000.0, 400000.0]              0.0\r\n",
        "3  (-2000.0, 400000.0]          21426.0\r\n",
        "4  (-2000.0, 400000.0]          41671.0\r\n",
        "'''\r\n",
        "# Import numpy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Specify the boundaries of the bins\r\n",
        "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\r\n",
        "\r\n",
        "# Bin labels\r\n",
        "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\r\n",
        "\r\n",
        "# Bin the continuous variable ConvertedSalary using these boundaries\r\n",
        "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], \r\n",
        "                                         bins, labels=labels)\r\n",
        "\r\n",
        "# Print the first 5 rows of the boundary_binned column\r\n",
        "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "           boundary_binned  ConvertedSalary\r\n",
        "    0      (-inf, 10000.0]              0.0\r\n",
        "    1  (50000.0, 100000.0]          70841.0\r\n",
        "    2      (-inf, 10000.0]              0.0\r\n",
        "    3   (10000.0, 50000.0]          21426.0\r\n",
        "    4   (10000.0, 50000.0]          41671.0\r\n",
        "\r\n",
        "<script.py> output:\r\n",
        "      boundary_binned  ConvertedSalary\r\n",
        "    0        Very low              0.0\r\n",
        "    1          Medium          70841.0\r\n",
        "    2        Very low              0.0\r\n",
        "    3             Low          21426.0\r\n",
        "    4             Low          41671.0\r\n",
        "'''\r\n",
        "'''\r\n",
        "Conclusion: Correct, now you can bin columns with equal spacing and predefined boundaries.\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}