{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 Building deep learning models with keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQHLuGyyMOVz3RaW2cD2iE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Deep%20Learning/DataCamp/Introduction%20to%20Deep%20Learning%20in%20Python/3%20Building%20deep%20learning%20models%20with%20keras/3_Building_deep_learning_models_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGgLHY1jyxKO"
      },
      "source": [
        "# Building deep learning models with keras\r\n",
        "\r\n",
        "In this chapter, you'll use the Keras library to build deep learning models for both regression and classification. You'll learn about the Specify-Compile-Fit workflow that you can use to make predictions, and by the end of the chapter, you'll have all the tools necessary to build deep neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXD3H02Kz0R1"
      },
      "source": [
        "# Creating a keras model\r\n",
        "\r\n",
        "1. Creating a keras model\r\n",
        "\r\n",
        "Congrats. You've learned the theory of back-propagation, which is core to understanding deep learning. Now you'll learn how to create and optimize these networks using the Keras interface to the TensorFlow deep learning library.\r\n",
        "2. Model building steps\r\n",
        "\r\n",
        "The Keras workflow has 4 steps. First, you specify the architecture, which is things like: how many layers do you want? how many nodes in each layer? What activation function do you want to use in each layer? Next, you compile the model. This specifies the loss function, and some details about how optimization works. Then you fit the model. Which is that cycle of back-propagation and optimization of model weights with your data. And finally you will want to use your model to make predictions. We'll go through these steps sequentially. The first step is creating or specifying your model.\r\n",
        "3. Model specification\r\n",
        "\r\n",
        "Here is the code to do that. This code has three blocks. First we import what we will need. Numpy is here only for reading some data. The other two imports are used for building our model. The second block of two lines reads the data. We read the data here so we can find the number of nodes in the input layer. That is stored as the variable n_cols. We always need to specify how many columns are in the input when building a keras model, because that is the number of nodes in the input layer. We then start building the model. The first line of model specification is model equals Sequential. There are two ways to build up a model, and we will focus on sequential, which is the easier way to build a model. Sequential models require that each layer has weights or connections only to the one layer coming directly after it in the network diagram. There are more exotic models out there with complex patterns of connections, but Sequential will do the trick for everything we need here. We start adding layers using the add method of the model. he type of layer you have seen, that standard layer type, is called a Dense layer. It is called Dense because all of the nodes in the previous layer connect to all of the nodes in the current layer. As you advance in deep learning, you may start using layers that aren't Dense. In each layer, we specify the number of nodes as the first positional argument, and the activation function we want to use in that layer using the keyword argument activation. Keras supports every activation function you will want in practice. In the first layer, we need to specify input shapes as shown here. That says the input will have n_cols columns, and there is nothing after the comma, meaning it can have any number of rows, that is, any number of data points. You'll notice the last layer has 1 node. That is the output layer, and it matches those diagrams where we ended with only a single node as the output or prediction of the model. This model has 2 hidden layers, and an output layer. You may be struck that each hidden layers has 100 nodes. Keras and TensorFlow do the math for us, so don't feel afraid to use much bigger networks than we've seen before. It's quite common to use 100 or 1000s nodes in a layer. You'll learn more about choosing an appropriate number of nodes later.\r\n",
        "4. Let's practice!\r\n",
        "\r\n",
        "For now, you can focus on how models are specified. It's time to practice that. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMy1z4f21cDI"
      },
      "source": [
        "# Understanding your data\r\n",
        "\r\n",
        "You will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it's good to understand your data by performing some exploratory analysis.\r\n",
        "\r\n",
        "The data is pre-loaded into a pandas DataFrame called df. Use the .head() and .describe() methods in the IPython Shell for a quick overview of the DataFrame.\r\n",
        "\r\n",
        "The target variable you'll be predicting is wage_per_hour. Some of the predictor variables are binary indicators, where a value of 1 represents True, and 0 represents False.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRf7WODbyeYI"
      },
      "source": [
        "In [1]:\r\n",
        "df.head()\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\r\n",
        "0           5.10      0              8              21   35       1     1   \r\n",
        "1           4.95      0              9              42   57       1     1   \r\n",
        "2           6.67      0             12               1   19       0     0   \r\n",
        "3           4.00      0             12               4   22       0     0   \r\n",
        "4           7.50      0             12              17   35       0     1   \r\n",
        "\r\n",
        "   south  manufacturing  construction  \r\n",
        "0      0              1             0  \r\n",
        "1      0              1             0  \r\n",
        "2      0              1             0  \r\n",
        "3      0              0             0  \r\n",
        "4      0              0             0  \r\n",
        "In [2]:\r\n",
        "df.describe().T\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "                count       mean        std   min    25%    50%    75%   max\r\n",
        "wage_per_hour   534.0   9.024064   5.139097   1.0   5.25   7.78  11.25  44.5\r\n",
        "union           534.0   0.179775   0.384360   0.0   0.00   0.00   0.00   1.0\r\n",
        "education_yrs   534.0  13.018727   2.615373   2.0  12.00  12.00  15.00  18.0\r\n",
        "experience_yrs  534.0  17.822097  12.379710   0.0   8.00  15.00  26.00  55.0\r\n",
        "age             534.0  36.833333  11.726573  18.0  28.00  35.00  44.00  64.0\r\n",
        "female          534.0   0.458801   0.498767   0.0   0.00   0.00   1.00   1.0\r\n",
        "marr            534.0   0.655431   0.475673   0.0   0.00   1.00   1.00   1.0\r\n",
        "south           534.0   0.292135   0.455170   0.0   0.00   0.00   1.00   1.0\r\n",
        "manufacturing   534.0   0.185393   0.388981   0.0   0.00   0.00   0.00   1.0\r\n",
        "construction    534.0   0.044944   0.207375   0.0   0.00   0.00   0.00   1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_W2Xc2X26lL"
      },
      "source": [
        "Of the 9 predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by .describe() will be informative here. How many binary indicator predictors are there?\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "1. 0.\r\n",
        " - Incorrect - there are some binary indicators in the data.\r\n",
        "\r\n",
        "2. 5.\r\n",
        " - Incorrect - Almost, but not quite.\r\n",
        "\r\n",
        "3. 6\r\n",
        " - Almost, but not quite."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM3TEAMu3OTg"
      },
      "source": [
        "# Specifying a model\r\n",
        "\r\n",
        "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\r\n",
        "\r\n",
        "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\r\n",
        "\r\n",
        "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\r\n",
        "\r\n",
        "For all exercises in this chapter, we've imported the Sequential model constructor, the Dense layer constructor, and pandas.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Store the number of columns in the predictors data to n_cols. This has been done for you.\r\n",
        "    \r\n",
        "2. Start by creating a Sequential model called model.\r\n",
        "\r\n",
        "3. Use the .add() method on model to add a Dense layer.\r\n",
        "\r\n",
        " - Add 50 units, specify activation='relu', and the input_shape parameter to be the tuple (n_cols,) which means it has n_cols items in each row of data, and any number of rows of data are acceptable as inputs.\r\n",
        "\r\n",
        "4. Add another Dense layer. This should have 32 units and a 'relu' activation.\r\n",
        "\r\n",
        "5. Finally, add an output layer, which is a Dense layer with a single node. Don't use any activation function here.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S3pgTCH3f6I"
      },
      "source": [
        "In [1]:\r\n",
        "df.head()\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  south  manufacturing  construction\r\n",
        "0           5.10      0              8              21   35       1     1      0              1             0\r\n",
        "1           4.95      0              9              42   57       1     1      0              1             0\r\n",
        "2           6.67      0             12               1   19       0     0      0              1             0\r\n",
        "3           4.00      0             12               4   22       0     0      0              0             0\r\n",
        "4           7.50      0             12              17   35       0     1      0              0             0\r\n",
        "\r\n",
        "In [2]:\r\n",
        "predictors\r\n",
        "Out[2]:\r\n",
        "\r\n",
        "array([[ 0,  8, 21, ...,  0,  1,  0], #This first row corresponds to the first row in df\r\n",
        "       [ 0,  9, 42, ...,  0,  1,  0],\r\n",
        "       [ 0, 12,  1, ...,  0,  1,  0],\r\n",
        "       ...,\r\n",
        "       [ 1, 17, 25, ...,  0,  0,  0],\r\n",
        "       [ 1, 12, 13, ...,  1,  0,  0],\r\n",
        "       [ 0, 16, 33, ...,  0,  1,  0]])\r\n",
        "\r\n",
        "In [3]:\r\n",
        "target # Just the first five I added manually instead of the whole array\r\n",
        "Out[3]:\r\n",
        "\r\n",
        "array([ 5.1 ,  4.95,  6.67,  4.  ,  7.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bivr2aQY5D3Q"
      },
      "source": [
        "# Import necessary modules\r\n",
        "import keras\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# Save the number of columns in predictors: n_cols\r\n",
        "n_cols = predictors.shape[1]\r\n",
        "\r\n",
        "# Set up the model: model\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# Add the first layer\r\n",
        "model.add(Dense(50, activation='relu', input_shape=(n_cols,)))\r\n",
        "\r\n",
        "# Add the second layer\r\n",
        "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\r\n",
        "\r\n",
        "# Add the output layer\r\n",
        "model.add(Dense(1))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7dcBNuB5GfM"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Well done! Now that you've specified the model, the next step is to compile it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fULG4Yo5Jya"
      },
      "source": [
        "# Compiling and fitting a model\r\n",
        "\r\n",
        "1. Compiling and fitting a model\r\n",
        "\r\n",
        "After you've specified a model, the next task is to compile it, which sets up the network for optimization, for instance creating an internal function to do back-propagation efficiently. The compile methods\r\n",
        "2. Why you need to compile your model\r\n",
        "\r\n",
        "has two important arguments for you to choose. The first is what optimizer to use, which controls the learning rate. In practice, the right choice of learning rate can make a big difference for how quickly our model finds good weights, and even how good a set of weights it can find. There are a few algorithms that automatically tune the learning rate. Even many experts in the field don't know all the details of all the optimization algorithms. So the pragmatic approach is to choose a versatile algorithm and use that for most problems. Adam is an excellent choice as your go-to optimizer. Adam adjusts the learning rate as it does gradient descent, to ensure reasonable values throughout the weight optimization process. The second thing you specify is the loss function. Mean squared error is the most common choice for regression problems. When we use keras for classification, you will learn a new default metric.\r\n",
        "3. Compiling a model\r\n",
        "\r\n",
        "Here is an example of the code to compile a model. It builds a model, as you've already seen, and then we add a compile command after building the model. After compiling the model, you can\r\n",
        "4. What is fitting a model\r\n",
        "\r\n",
        "fit it. That is applying back-propagation and gradient descent with your data to update the weights. The fit step looks similar to what you've seen in scikit-learn, though it has more options which we will explore soon. Even with the Adam optimizer, which is pretty smart, it can improve your optimization process if you scale all the data so each feature is, on average, about similar sized values. One common approach is to subtract each feature by that features mean, and divide it by it's standard deviation.\r\n",
        "5. Fitting a model\r\n",
        "\r\n",
        "You can see what the code looks like here. After the compile step, we run fit, with the predictors as the first argument. When you run this, you will see some output showing the optimizations progress as it fits the data. We'll go into more detail about this output soon, but for now,\r\n",
        "6. Let's practice!\r\n",
        "\r\n",
        "just think of it as a log showing model performance on the training data as we update model weights. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DqFhwWv8ISy"
      },
      "source": [
        "# Compiling the model\r\n",
        "\r\n",
        "You're now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers [here](https://keras.io/optimizers/#adam), and if you are really curious to learn more, you can read the [original paper](https://arxiv.org/abs/1412.6980v8) that introduced the Adam optimizer.\r\n",
        "\r\n",
        "In this exercise, you'll use the Adam optimizer and the mean squared error loss function. Go for it!\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Compile the model using model.compile(). Your optimizer should be 'adam' and the loss should be 'mean_squared_error'.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLHcUnpf8_Ke"
      },
      "source": [
        "# Import necessary modules\r\n",
        "import keras\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# Specify the model\r\n",
        "n_cols = predictors.shape[1]\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\r\n",
        "model.add(Dense(32, activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "\r\n",
        "# Verify that model contains information from compiling\r\n",
        "print(\"Loss function: \" + model.loss)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Loss function: mean_squared_error\r\n",
        "\r\n",
        "Conclusion:\r\n",
        "\r\n",
        "Fantastic work - all that's left now is to fit the model!\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcA3PgRz9dz3"
      },
      "source": [
        "# Fitting the model\r\n",
        "\r\n",
        "You're at the most fun part. You'll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called predictors and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Fit the model. Remember that the first argument is the predictive features (predictors), and the data to be predicted (target) is the second argument.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnVikClU9i7I"
      },
      "source": [
        "# Import necessary modules\r\n",
        "import keras\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "\r\n",
        "# Specify the model\r\n",
        "n_cols = predictors.shape[1]\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\r\n",
        "model.add(Dense(32, activation='relu'))\r\n",
        "model.add(Dense(1))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(predictors, target)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Epoch 1/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 146.0927\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "320/534 [================>.............] - ETA: 0s - loss: 103.6188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 80.2264      \r\n",
        "    Epoch 2/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 86.2792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "416/534 [======================>.......] - ETA: 0s - loss: 31.3256\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 30.6174     \r\n",
        "    Epoch 3/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 21.0396\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "384/534 [====================>.........] - ETA: 0s - loss: 28.6841\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 27.1464     \r\n",
        "    Epoch 4/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 16.9354\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "480/534 [=========================>....] - ETA: 0s - loss: 26.4622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 25.1513     \r\n",
        "    Epoch 5/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 23.2456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "448/534 [========================>.....] - ETA: 0s - loss: 21.7948\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 24.0650     \r\n",
        "    Epoch 6/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 13.3707\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "512/534 [===========================>..] - ETA: 0s - loss: 23.0862\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 23.2748     \r\n",
        "    Epoch 7/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 28.1830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 22.5775     \r\n",
        "    Epoch 8/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 11.5195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 22.1685     \r\n",
        "    Epoch 9/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 21.9095\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 21.7670     \r\n",
        "    Epoch 10/10\r\n",
        "    \r\n",
        " 32/534 [>.............................] - ETA: 0s - loss: 5.4847\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "534/534 [==============================] - 0s - loss: 21.5618   \r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT2gp89E99rK"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Superb! You now know how to specify, compile, and fit a deep learning model using keras!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqdhicZe-upG"
      },
      "source": [
        "# Classification models\r\n",
        "\r\n",
        "1. Classification models\r\n",
        "\r\n",
        "So far we have focused on regression models. But deep learning works similarly for classification, that is for predicting outcomes from a set of discrete options.\r\n",
        "2. Classification\r\n",
        "\r\n",
        "For classification, you do a couple of things differently. The biggest changes are:1, set the loss function as 'categorical_crossentropy' instead of 'mean_squared_error'. This isn't the only possible loss function for classification problems, but it is by far the most common. You may have heard of this before under the name LogLoss. We won't go into the mathematics of categorical crossentropy here. For categorical crossentropy loss function, a lower score is better. But it's still hard to interpret. So I've added this argument \"metrics equals accuracy\". This means I want to print out the accuracy score at the end of each epoch, which makes it easier to see and understand the models progress. Second you need to modify the last layer, so it has a separate node for each potential outcome. You will also change the activation function to softmax. The softmax activation function ensures the predictions sum to 1, so they can be interpreted like probabilities.\r\n",
        "3. Quick look at the data\r\n",
        "\r\n",
        "Here is some data for a binary classification problem. We have data from the NBA basketball league. It includes facts about each shot, and the\r\n",
        "4. Quick look at the data\r\n",
        "\r\n",
        "shot result is either 0 or 1, indicating whether the shot went in or not. The outcome here is in a single column, which is not uncommon. But in general, we'll want to convert categoricals in Keras to a format with a separate column for each output. Keras includes a function to do that, which you will see in the code soon. This setup is consistent with the fact that your model will have a separate node in the output for each possible class.\r\n",
        "5. Transforming to categorical\r\n",
        "\r\n",
        "We have a new column for each value of shot_result. A 1 in any column indicates that this column corresponds to the value from the original data. This is sometimes called one-hot encoding. If the original data had 3 or 4 or 100 different values, the new array for our data would have 3 or 4 or 100 columns respectively.\r\n",
        "6. Classification\r\n",
        "\r\n",
        "Here is the code to build a model with that data. First, we import that utility function to convert the data from one column to multiple columns. That is this function to_categorical. We then read in the data. I like reading in the data with pandas, in case I want to inspect it. But this could be done with numpy. I also do a couple of pandas tricks here which you may or may not be familiar with. Here I use the drop command to get a version of my data without the target column. I then store that as a numpy matrix. And here, I use a dot notation to access the column with the prediction target. We keep all the data except the outcome in a matrix called predictors. We then create our target using the to_categorical function. Then we build our model. It looks similar to models you've seen. Except the last line of the model definition has 2 nodes, for the 2 possible outcomes. And it has the softmax activation function.\r\n",
        "7. Classification\r\n",
        "\r\n",
        "Lets look at the results now. Both accuracy and loss improve measurably for the first 3 epochs, and then the improvement slows down. Sometimes it gets a little worse for an epoch, sometimes it gets a little better. We will soon see a more sophisticated way to determine how long to train, but training for 10 epochs got us to that flat part of the loss function, so this worked well in this case. -\r\n",
        "8. Let's practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBEYI-4_aee"
      },
      "source": [
        "# Understanding your classification data\r\n",
        "\r\n",
        "Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as age, fare and where each passenger embarked from to predict who will survive. This data is from [a tutorial on data science competitions](https://www.kaggle.com/c/titanic). Look [here](https://www.kaggle.com/c/titanic/data) for descriptions of the features.\r\n",
        "\r\n",
        "The data is pre-loaded in a pandas DataFrame called df.\r\n",
        "\r\n",
        "It's smart to review the maximum and minimum values of each variable to ensure the data isn't misformatted or corrupted. What was the maximum age of passengers on the Titanic? Use the .describe() method in the IPython Shell to answer this question.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp0f-q-4BvIX"
      },
      "source": [
        "In [1]:\r\n",
        "df.describe().T\r\n",
        "Out[1]:\r\n",
        "\r\n",
        "                           count       mean        std   min      25%        50%   75%       max\r\n",
        "survived                   891.0   0.383838   0.486592  0.00   0.0000   0.000000   1.0    1.0000\r\n",
        "pclass                     891.0   2.308642   0.836071  1.00   2.0000   3.000000   3.0    3.0000\r\n",
        "age                        891.0  29.699118  13.002015  0.42  22.0000  29.699118  35.0   80.0000\r\n",
        "sibsp                      891.0   0.523008   1.102743  0.00   0.0000   0.000000   1.0    8.0000\r\n",
        "parch                      891.0   0.381594   0.806057  0.00   0.0000   0.000000   0.0    6.0000\r\n",
        "fare                       891.0  32.204208  49.693429  0.00   7.9104  14.454200  31.0  512.3292\r\n",
        "male                       891.0   0.647587   0.477990  0.00   0.0000   1.000000   1.0    1.0000\r\n",
        "embarked_from_cherbourg    891.0   0.188552   0.391372  0.00   0.0000   0.000000   0.0    1.0000\r\n",
        "embarked_from_queenstown   891.0   0.086420   0.281141  0.00   0.0000   0.000000   0.0    1.0000\r\n",
        "embarked_from_southampton  891.0   0.722783   0.447876  0.00   0.0000   1.000000   1.0    1.0000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hzSZGMkB5ls"
      },
      "source": [
        "Possible Answers\r\n",
        "\r\n",
        "1. 29.699.\r\n",
        " - Incorrect. You seem to be looking at the median value of age.\r\n",
        "\r\n",
        "2. 80.\r\n",
        " - Incorrect - it is indeed listed.\r\n",
        " \r\n",
        "3. 891.\r\n",
        " - Incorrect - you seem to be looking at the total number of entries in the data.\r\n",
        "\r\n",
        "4. It is not listed.\r\n",
        " - Incorrect - it is indeed listed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxhP9FvsC0ym"
      },
      "source": [
        "# Last steps in classification models\r\n",
        "\r\n",
        "You'll now create a classification model using the titanic dataset, which has been pre-loaded into a DataFrame called df. You'll take information about the passengers and predict which ones survived.\r\n",
        "\r\n",
        "The predictive variables are stored in a NumPy array predictors. The target to predict is in df.survived, though you'll have to manipulate it for keras. The number of predictive features is stored in n_cols.\r\n",
        "\r\n",
        "Here, you'll use the 'sgd' optimizer, which stands for [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). You'll learn more about this in the next chapter!\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Convert df.survived to a categorical variable using the to_categorical() function.\r\n",
        "    \r\n",
        "2. Specify a Sequential model called model.\r\n",
        "    \r\n",
        "3. Add a Dense layer with 32 nodes. Use 'relu' as the activation and (n_cols,) as the input_shape.\r\n",
        "    \r\n",
        "4. Add the Dense output layer. Because there are two outcomes, it should have 2 units, and because it is a classification model, the activation should be 'softmax'.\r\n",
        "\r\n",
        "5. Compile the model, using 'sgd' as the optimizer, 'categorical_crossentropy' as the loss function, and metrics=['accuracy'] to see the accuracy (what fraction of predictions were correct) at the end of each epoch.\r\n",
        "    \r\n",
        "6. Fit the model using the predictors and the target.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvK2ZnNgD9qc"
      },
      "source": [
        "# Import necessary modules\r\n",
        "import keras\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.utils import to_categorical\r\n",
        "\r\n",
        "# Convert the target to categorical: target\r\n",
        "target = to_categorical(df.survived)\r\n",
        "\r\n",
        "# Set up the model\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "# Add the first layer\r\n",
        "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\r\n",
        "\r\n",
        "# Add the output layer\r\n",
        "model.add(Dense(2, activation='softmax'))\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fit the model\r\n",
        "model.fit(predictors, target)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Epoch 1/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 1s - loss: 7.6250 - acc: 0.2188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "384/891 [===========>..................] - ETA: 0s - loss: 3.3985 - acc: 0.5651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "832/891 [===========================>..] - ETA: 0s - loss: 2.4669 - acc: 0.6046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 2.5525 - acc: 0.5971     \r\n",
        "    Epoch 2/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 2.0159 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "416/891 [=============>................] - ETA: 0s - loss: 1.9297 - acc: 0.5649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/891 [=========================>....] - ETA: 0s - loss: 1.6602 - acc: 0.5750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 1.6544 - acc: 0.5657     \r\n",
        "    Epoch 3/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.7604 - acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/891 [=================>............] - ETA: 0s - loss: 1.2112 - acc: 0.6287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.9947 - acc: 0.6566     \r\n",
        "    Epoch 4/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.7286 - acc: 0.5625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/891 [=================>............] - ETA: 0s - loss: 0.7557 - acc: 0.6471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.8037 - acc: 0.6476     \r\n",
        "    Epoch 5/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.6628 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "576/891 [==================>...........] - ETA: 0s - loss: 0.6649 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6792 - acc: 0.6420     \r\n",
        "    Epoch 6/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.4690 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "640/891 [====================>.........] - ETA: 0s - loss: 0.6079 - acc: 0.6781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6267 - acc: 0.6723     \r\n",
        "    Epoch 7/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.6455 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "768/891 [========================>.....] - ETA: 0s - loss: 0.6427 - acc: 0.6901\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6322 - acc: 0.7003     \r\n",
        "    Epoch 8/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.5426 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "672/891 [=====================>........] - ETA: 0s - loss: 0.6384 - acc: 0.6801\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6300 - acc: 0.6891     \r\n",
        "    Epoch 9/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.6572 - acc: 0.6250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/891 [=================>............] - ETA: 0s - loss: 0.6139 - acc: 0.6820\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6004 - acc: 0.6902     \r\n",
        "    Epoch 10/10\r\n",
        "    \r\n",
        " 32/891 [>.............................] - ETA: 0s - loss: 0.4791 - acc: 0.7500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/891 [=================>............] - ETA: 0s - loss: 0.6385 - acc: 0.6728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "891/891 [==============================] - 0s - loss: 0.6461 - acc: 0.6723 \r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mpyq13EDNlw"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Fantastic! This simple model is generating an accuracy of 68!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKEWBAF1EJC2"
      },
      "source": [
        "# Using models\r\n",
        "\r\n",
        "1. Using models\r\n",
        "\r\n",
        "Now that you can build basic deep learning models, I'll show you how to use them. Then we'll go into some finer details on fine tuning model architectures. The things you'll want to do in order to use these models\r\n",
        "2. Using models\r\n",
        "\r\n",
        "are: save a model after you've trained it, reload that model, make predictions with the model.\r\n",
        "3. Saving, reloading and using your Model\r\n",
        "\r\n",
        "Here is the code to save a model, reload it, and make predictions. We've imported a load_model function here. Once I have a model I want to save, I can save it with the \"save\" method. I supply a filename. Models are saved in a format called hdf5, for which h5 is the common extension. I then load the model back into memory with the load_model function here. I then make predictions. The model I've loaded here is a classification model. The predictions come in the same format as the prediction target. You may recall that this had 1 column for whether the shot was missed, and then a 2nd column for whether the shot was made. In practice, I probably only want the probability that the shot is made. So, I'll extract that second column with numpy indexing, and I called that probability_true. Lastly, sometimes I'll want to verify that the model I loaded has the same structure I expect.\r\n",
        "4. Verifying model structure\r\n",
        "\r\n",
        "You can print out a summary of the model architecture with the summary method. You can see the output here. Now that you can save your model, reload it, make predictions, and verify its structure, you have most of what you need to not just build models, but to work with them in practical situations.\r\n",
        "5. Let's practice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjGio-ZyFKn1"
      },
      "source": [
        "Making predictions\r\n",
        "\r\n",
        "The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data.\r\n",
        "\r\n",
        "In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create your predictions using the model's .predict() method on pred_data.\r\n",
        "\r\n",
        "2. Use NumPy indexing to find the column corresponding to predicted probabilities of survival being True. This is the second column (index 1) of predictions. Store the result in predicted_prob_true and print it.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmYEmvihFqGx"
      },
      "source": [
        "# Specify, compile, and fit the model\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\r\n",
        "model.add(Dense(2, activation='softmax'))\r\n",
        "model.compile(optimizer='sgd', \r\n",
        "              loss='categorical_crossentropy', \r\n",
        "              metrics=['accuracy'])\r\n",
        "model.fit(predictors, target)\r\n",
        "\r\n",
        "# Calculate predictions: predictions\r\n",
        "predictions = model.predict(pred_data)\r\n",
        "\r\n",
        "# Calculate predicted probability of survival: predicted_prob_true\r\n",
        "predicted_prob_true = predictions[:,1]\r\n",
        "\r\n",
        "# print predicted_prob_true\r\n",
        "print(predicted_prob_true)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Epoch 1/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 5.3679 - acc: 0.3438\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "608/800 [=====================>........] - ETA: 0s - loss: 2.4874 - acc: 0.5658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 2.4836 - acc: 0.5737     \r\n",
        "    Epoch 2/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 2.0022 - acc: 0.5625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "704/800 [=========================>....] - ETA: 0s - loss: 1.3517 - acc: 0.6151\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 1.2657 - acc: 0.6275     \r\n",
        "    Epoch 3/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.5930 - acc: 0.7812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "768/800 [===========================>..] - ETA: 0s - loss: 0.7645 - acc: 0.6510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.7709 - acc: 0.6462     \r\n",
        "    Epoch 4/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.6133 - acc: 0.6875\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.7582 - acc: 0.6575     \r\n",
        "    Epoch 5/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.5856 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "608/800 [=====================>........] - ETA: 0s - loss: 0.6061 - acc: 0.7023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6240 - acc: 0.6812     \r\n",
        "    Epoch 6/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.6368 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "576/800 [====================>.........] - ETA: 0s - loss: 0.6267 - acc: 0.6771\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6282 - acc: 0.6825     \r\n",
        "    Epoch 7/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.5497 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/800 [===================>..........] - ETA: 0s - loss: 0.6942 - acc: 0.6746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6604 - acc: 0.6900     \r\n",
        "    Epoch 8/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.6141 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "512/800 [==================>...........] - ETA: 0s - loss: 0.6625 - acc: 0.6562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6596 - acc: 0.6675     \r\n",
        "    Epoch 9/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.6784 - acc: 0.7188\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "544/800 [===================>..........] - ETA: 0s - loss: 0.6424 - acc: 0.6710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6240 - acc: 0.6825     \r\n",
        "    Epoch 10/10\r\n",
        "    \r\n",
        " 32/800 [>.............................] - ETA: 0s - loss: 0.7318 - acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "736/800 [==========================>...] - ETA: 0s - loss: 0.6126 - acc: 0.6821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
        "800/800 [==============================] - 0s - loss: 0.6130 - acc: 0.6850     \r\n",
        "    \r\n",
        "    [0.25623408 0.45260283 0.7697866  0.5138137  0.2441603  0.21559338\r\n",
        "     0.10678288 0.3734369  0.19850041 0.54136074 0.26264063 0.34593353\r\n",
        "     0.20412605 0.41284004 0.2223513  0.16609463 0.31174162 0.44413403\r\n",
        "     0.12365615 0.4105931  0.6191477  0.26056415 0.11202551 0.35903063\r\n",
        "     0.4322243  0.21172646 0.5325234  0.5315704  0.21760306 0.5478295\r\n",
        "     0.46044198 0.46574932 0.22949217 0.29277655 0.37371725 0.6278235\r\n",
        "     0.33704054 0.21773814 0.5422688  0.4534816  0.3345991  0.4255777\r\n",
        "     0.47968638 0.19282793 0.39832142 0.13464618 0.3629767  0.20013686\r\n",
        "     0.45633298 0.69206667 0.38925478 0.0309085  0.46303532 0.57084596\r\n",
        "     0.29680806 0.4206786  0.8768284  0.24258076 0.4630609  0.22949217\r\n",
        "     0.16100885 0.35804772 0.27240106 0.38402492 0.36700103 0.18236724\r\n",
        "     0.35253486 0.5328016  0.22925237 0.44043037 0.26272342 0.46174735\r\n",
        "     0.18041034 0.11512452 0.4610374  0.44015813 0.3799623  0.34876058\r\n",
        "     0.2165958  0.5657592  0.46994975 0.19993727 0.3773236  0.28283578\r\n",
        "     0.26410118 0.47623226 0.33496824 0.5200377  0.44110638 0.46630716\r\n",
        "     0.20740111]\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku9-OxcgFyJx"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Excellent work! You're now ready to begin learning how to fine-tune your models."
      ]
    }
  ]
}