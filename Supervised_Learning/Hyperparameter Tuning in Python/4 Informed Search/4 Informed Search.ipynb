{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 Informed Search.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcrGS86Vc3hG/Ttj7qwzv+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Supervised_Learning/Hyperparameter%20Tuning%20in%20Python/4%20Informed%20Search/4%20Informed%20Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijyA3DJ9IU6g"
      },
      "source": [
        "# Informed Search\r\n",
        "\r\n",
        "In this final chapter you will be given a taste of more advanced hyperparameter tuning methodologies known as ''informed search''. This includes a methodology known as Coarse To Fine as well as Bayesian & Genetic hyperparameter tuning algorithms. You will learn how informed search differs from uninformed search and gain practical skills with each of the mentioned methodologies, comparing and contrasting them as you go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Mlpu4NNdlh"
      },
      "source": [
        "# Informed Search: Coarse to Fine\r\n",
        "\r\n",
        "1. Informed Search: Coarse to Fine\r\n",
        "In this final chapter, we will look at a group of hyperparameter tuning techniques known as informed search. We will begin with a basic informed search methodology, Coarse to Fine.\r\n",
        "\r\n",
        "2. Informed vs Uninformed Search\r\n",
        "So far, everything we have done with grid and random search were instances of uninformed search. When performing uninformed search, each iteration of hyperparameter tuning (each model) does not learn from the previous iterations. This allows us to undertake hyperparameter tuning in parallel. However, doing so much work before being able to learn and iterate seems a bit inefficient, doesn't it?\r\n",
        "\r\n",
        "3. Informed vs Uninformed\r\n",
        "Let's make it more explicit. So far, we have been creating all the models at once and collating their scores before deciding the best at the end. An alternate approach would be to build models sequentially, learning from each iteration. This is what undertaking hyperparameter searching in an informed way allows us to do.\r\n",
        "\r\n",
        "4. Coarse to Fine Tuning\r\n",
        "A first, basic, informed search methodology is known as 'Coarse to Fine tuning'. The name comes from the fact that you start out with a rough approach and iteratively refine your hyperparameter search. The process goes as follows. First, undertake a random search. Second, review results to see promising areas in your hyperparameter search space. Then, undertake a grid search in the smaller area. Continue this process until an optimal score is obtained or the area becomes too small (meaning not many hyperparameter values) to properly search. You could also substitute step 3 with further random searches before the grid search.\r\n",
        "\r\n",
        "5. Why Coarse to Fine?\r\n",
        "Coarse to Fine tuning optimizes the advantages of grid and random search. It uses the advantages of both methods. The wide searching abilities of random search and including a grid search for deeper coverage at the relevant time. It therefore better utilizes efforts so you can iterate. If you see one area on the grid is producing poor results, you will undertake your next search elsewhere, saving you effort. Also note that Coarse to fine is not informed on every model, but on batches of models. Still, it is more informed than what we have done so far!\r\n",
        "\r\n",
        "6. Undertaking Coarse to Fine\r\n",
        "Let us set up a pretty large hyperparameter search over these following ranges. How many possible hyperparameter combinations are there? If we create the combinations as we did in previous exercises we can see 134,400 possible combinations!\r\n",
        "\r\n",
        "7. Visualizing Coarse to Fine\r\n",
        "Rather than running all those models, we can do a random search on just 500 models Let's first visualize just the accuracy column as a density plot. Wow! There were certainly some bad models in there. Good thing we didn't do a huge grid search and waste our time on those! But there are some good models in there, what hyperparameters did they use?\r\n",
        "\r\n",
        "8. Visualizing Coarse to Fine\r\n",
        "Here is a snapshot of the top models of the results DataFrame. Hmmmm, that's not very informative about what to do next, is it? Perhaps the min_samples_leaf being 7? But we don't know if there were also bad models with this same value.\r\n",
        "\r\n",
        "9. Visualizing Coarse to Fine\r\n",
        "Instead, let's visualize each hyperparameter against the accuracy scores to see if we can see any trends. We will do a simple scatter plot using a results data frame. That's more interesting! See how accuracy tends to be lower when max_depth is below 6 or 7 and above 30?\r\n",
        "\r\n",
        "10. Visualizing coarse to Fine\r\n",
        "A similar exercise with the other two hyperparameters finds some interesting insights. The min_samples_leaf being a bit better below 8, and the learn_rate seems worse above 1-point-3. Now we have a plan for our next iteration.\r\n",
        "\r\n",
        "11. The next steps\r\n",
        "Let's summarize what we learned from the first grid search. Values of max_depth between 8 and 30, learn_rate of less than 1-point-3 and perhaps min_samples_leaf less than 8 tend to have higher accuracies. You can take this and undertake a more refined grid search or even another random search around these values to improve the model This simple, bivariate graphing was insightful, but using more advanced visualization techniques could also prove useful.\r\n",
        "\r\n",
        "12. Let's practice!\r\n",
        "Let's practice undertaking a Coarse to Fine search!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYJwJ_u9M78S"
      },
      "source": [
        "# Visualizing Coarse to Fine\r\n",
        "\r\n",
        "You're going to undertake the first part of a Coarse to Fine search. This involves analyzing the results of an initial random search that took place over a large search space, then deciding what would be the next logical step to make your hyperparameter search finer.\r\n",
        "\r\n",
        "You have available:\r\n",
        "\r\n",
        " - combinations_list - a list of the possible hyperparameter combinations the random search was undertaken on.\r\n",
        " - results_df - a DataFrame that has each hyperparameter combination and the resulting accuracy of all 500 trials. Each hyperparameter is a column, with the header the hyperparameter name.\r\n",
        " - visualize_hyperparameter() - a function that takes in a column of the DataFrame (as a string) and produces a scatter plot of this column's values compared to the accuracy scores. An example call of the function would be visualize_hyperparameter('accuracy')\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Confirm (by printing out) the size of the combinations_list, justifying the need to start with a random search.\r\n",
        "\r\n",
        "2. Sort the results_df by accuracy values and print the top 10 rows. Are there clear insights? Beware a small sample size!\r\n",
        "\r\n",
        "3. Confirm (by printing out) which hyperparameters were used in this search. These are the column names in results_df.\r\n",
        "\r\n",
        "4. Call visualize_hyperparameter() with each hyperparameter in turn (max_depth, min_samples_leaf, learn_rate). Are there any trends?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTN8QcRxG-ri"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTlOIsSCNqq2"
      },
      "source": [
        "# Coarse to Fine Iterations\r\n",
        "\r\n",
        "You will now visualize the first random search undertaken, construct a tighter grid and check the results. You will have available:\r\n",
        "\r\n",
        " - results_df - a DataFrame that has the hyperparameter combination and the resulting accuracy of all 500 trials. Only the hyperparameters that had the strongest visualizations from the previous exercise are included (max_depth and learn_rate)\r\n",
        " - `visualize_first()` - This function takes no arguments but will visualize each of your hyperparameters against accuracy for your first random search.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJNkkXsON8J7"
      },
      "source": [
        "'''\r\n",
        "In [2]:\r\n",
        "??visualize_first()\r\n",
        "Signature: visualize_first()\r\n",
        "Source:\r\n",
        "def visualize_first():\r\n",
        "  for name in results_df.columns[0:2]:\r\n",
        "    plt.clf()\r\n",
        "    plt.scatter(results_df[name],results_df['accuracy'], c=['blue']*500)\r\n",
        "    plt.gca().set(xlabel='{}'.format(name), ylabel='accuracy', title='Accuracy for different {}s'.format(name))\r\n",
        "    plt.gca().set_ylim([0,100])\r\n",
        "    x_line = 20\r\n",
        "    if name == \"learn_rate\":\r\n",
        "        x_line = 1\r\n",
        "    plt.axvline(x=x_line, color=\"red\", linewidth=4)\r\n",
        "    plt.show()\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHloceN-P2z6"
      },
      "source": [
        "Instructions\r\n",
        "\r\n",
        "1. Use the visualize_first() function to check the values of max_depth and learn_rate that tend to perform better. A convenient red line will be added to make this explicit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUrfXqKnQDDn"
      },
      "source": [
        "# Informed Search: Bayesian Statistics\r\n",
        "\r\n",
        "1. Informed Methods: Bayesian Statistics\r\n",
        "Now we will explore an important statistical concept and its application to informed hyperparameter tuning: Bayesian methods.\r\n",
        "\r\n",
        "2. Bayes Introduction\r\n",
        "Bayes rule is a famous statistical tool that has been around for 250 years. So how can this be relevant to machine learning today? Well, Bayes rule is actually a method where we can iteratively use new evidence to update our beliefs about an outcome. Intuitively this makes senses as when using informed search we want to learn from evidence to make our hyperparameter tuning better.\r\n",
        "\r\n",
        "3. Bayes Rule\r\n",
        "Here is the Bayes Rule formula. The left hand side is the probability of A (which we care about), given that B has occurred, where B is some new (relevant) evidence. This is known as the 'posterior'. The right hand side is how we calculate this. P(A) is known as the 'prior'. It is the initial hypothesis about the event we care about. See how it is different to P(A|B)? The latter is the probability given new evidence.\r\n",
        "\r\n",
        "4. Bayes Rule\r\n",
        "P(B) is the 'marginal likelihood' and it is the probability of observing this new evidence. P(B|A) is the 'likelihood' which is the probability of observing the evidence, given the event we care about. This all may be quite confusing, but let's use a common example of a medical diagnosis to demonstrate.\r\n",
        "\r\n",
        "5. Bayes in Medicine\r\n",
        "Let's take a medical example to illustrate the Bayesian process. Let's say 5% of people in the general population have a certain disease. This is our P(D). 10% of people are genetically predisposed to this condition. That is, because of their genetics, they are more likely to get this condition. This is our P(Pre). We also know that 20% of people with the disease are predisposed which is our P(Pre|D).\r\n",
        "\r\n",
        "6. Bayes in Medicine\r\n",
        "So what is the probability that any given person has the disease? If we know nothing about a person, then the probability of them having the disease is just the prior. However, what if we add some new evidence that this person is predisposed? We can update our beliefs by subbing into Bayes formula. Now we can see that the probability of having the disease, given that someone is predisposed is around 10%, or double the original prior.\r\n",
        "\r\n",
        "7. Bayes in Hyperparameter Tuning\r\n",
        "We can apply this logic to hyperparameter tuning using the following process. Pick a hyperparameter combination. Build a model. Get new _evidence_ (the score of the model). Update our beliefs and chose better hyperparameters next round and continue until we are happy with the result. Bayesian hyperparameter tuning is very new but quite popular for larger and more complex hyperparameter tuning tasks.\r\n",
        "\r\n",
        "8. Bayesian Hyperparameter Tuning with Hyperopt\r\n",
        "A useful package for Bayesian hyperparameter tuning is Hyperopt. To undertake Bayesian hyperparameter tuning with this package, we first need to set the domain, which is our Grid, with a bit of a twist. Then, we set the optimization algorithm (we will use the default TPE). Finally, we need to set the objective function to minimize: we will use 1-Accuracy because this package tries to minimize not maximize something.\r\n",
        "\r\n",
        "9. Hyperopt: Set the Domain (grid)\r\n",
        "There are many options for how to lay out the grid in Bayesian optimization including those mentioned on the slide. Hyperopt does not use point values on the grid but instead each point represents probabilities for each hyperparameter value. To keep it simple, we will use uniform distribution. There are many more distributions if you check the documentation.\r\n",
        "\r\n",
        "10. The Domain\r\n",
        "Let's see how to set up our grid or domain in hyperopt. This code demonstrates this using a simple uniform distribution between the min and max values supplied. quniform means uniform but quantized (or binned) by the specified third number.\r\n",
        "\r\n",
        "11. The objective function\r\n",
        "We need to define an objective function to run the algorithm. It needs to take in the parameters to test and use those to create an estimator (for us a GBM). The estimator is cross-validated to find the best average score and returns the average loss over the folds. We need to make our loss as one minus the best score since Hyperopt will work to minimize what we return - we don't want to minimize accuracy! I have also written a small function to write out the results at each iteration for analysis later.\r\n",
        "\r\n",
        "12. Run the algorithm\r\n",
        "Now we simply need to call the algorithm. We give it the objective function we created, the sample space we set up, how many iterations and an optional random seed to ensure consistency. The algorithm is the standard algorithm and best one currently implemented. This function will only return the best hyperparameters which is why we logged out results at each iteration.\r\n",
        "\r\n",
        "13. Let's practice!\r\n",
        "Let's practice Bayesian hyperparameter tuning in python!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlaYm9_nQTa8"
      },
      "source": [
        "# Bayes Rule in Python\r\n",
        "\r\n",
        "In this exercise you will undertake a practical example of setting up Bayes formula, obtaining new evidence and updating your 'beliefs' in order to get a more accurate result. The example will relate to the likelihood that someone will close their account for your online software product.\r\n",
        "\r\n",
        "These are the probabilities we know:\r\n",
        "\r\n",
        " - 7% (0.07) of people are likely to close their account next month\r\n",
        " - 15% (0.15) of people with accounts are unhappy with your product (you don't know who though!)\r\n",
        " - 35% (0.35) of people who are likely to close their account are unhappy with your product\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Assign the different probabilities (as decimals) to variables. p_unhappy is the likelihood someone is unhappy, p_unhappy_close is the probability that someone is unhappy with the product, given they are going to close their account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qxYbXhxQso4"
      },
      "source": [
        "# Bayesian Hyperparameter tuning with Hyperopt\r\n",
        "\r\n",
        "In this example you will set up and run a bayesian hyperparameter optimization process using the package Hyperopt (already imported as hp for you). You will set up the domain (which is similar to setting up the grid for a grid search), then set up the objective function. Finally, you will run the optimizer over 20 iterations.\r\n",
        "\r\n",
        "You will need to set up the domain using values:\r\n",
        "\r\n",
        " - max_depth using quniform distribution (between 2 and 10, increasing by 2)\r\n",
        " - learning_rate using uniform distribution (0.001 to 0.9)\r\n",
        "\r\n",
        "Note that for the purpose of this exercise, this process was reduced in data sample size and hyperopt & GBM iterations. If you are trying out this method by yourself on your own machine, try a larger search space, more trials, more cvs and a larger dataset size to really see this in action!\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Set up a space dictionary using the domain mentioned above.\r\n",
        "\r\n",
        "2. Set up the objective function using a gradient boosting classifier.\r\n",
        "\r\n",
        "3. Run the algorithm for 20 evaluations (just use the default, suggested algorithm from the slides)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm0eVxynRBe0"
      },
      "source": [
        "# Informed Search: Genetic Algorithms\r\n",
        "\r\n",
        "1. Informed Methods: Genetic Algorithms\r\n",
        "Let's learn about another informed search methodology using genetic hyperparameter tuning.\r\n",
        "\r\n",
        "2. A lesson on genetics\r\n",
        "To understand genetic algorithms in machine learning, we should firstly understand how the inspiration for this technology, biological evolution, works. In genetic evolution in the real world, we have the following process. To begin, there are many creatures existing called offspring. The strongest creatures survive the tough environment and pair off. There is some 'crossover' as they form offspring. Random mutations occur with some of the offspring. These mutations sometimes help give some offspring an advantage. Finally, go back to (1)! This is how evolution works in nature.\r\n",
        "\r\n",
        "3. Genetics in Machine Learning\r\n",
        "We can apply the same idea to hyperparameter tuning. Firstly we can create some models (that have hyperparameter settings). Second, we can pick the best (by our scoring function). These are the ones that 'survive'. We can then create new models that are similar to the best ones. We add in some randomness so we don't reach a local optimum. Finally we continue this until we are happy!\r\n",
        "\r\n",
        "4. Why does this work well?\r\n",
        "This is an informed search that has a number of advantages. It allows us to learn from previous iterations, just like Bayesian hyperparameter tuning. It has the additional advantage of some randomness. This randomness is important because it means we won't just be working on finding similar models and going down a singular path. We have a chance to move to a completely different area of the hyperparameter search space which may be better. Finally, It takes care of many tedious aspects of machine learning like algorithm and hyperparameter choice.\r\n",
        "\r\n",
        "5. Introducing TPOT\r\n",
        "A useful library for genetic hyperparameter tuning is TPOT. The documentation on the website explains its vision. It aims to be your Data Science Assistant, automatically optimizing pipelines of models using genetic programming.\" This is great because pipelines not only include the model (or multiple models) but also work on features and other aspects of the process. Plus it returns the Python code of the pipeline for you!\r\n",
        "\r\n",
        "6. TPOT components\r\n",
        "The key arguments to a TPOT classifier are: generations: The number of cycles we undertake of creating offspring models, mutating and crossing over, picking the best and continuing. population_size: In each iteration, this is how many models we keep. The strongest 'offspring'. offspring_size: In each iteration this is the number of offspring (for us, models) that we create. mutation_rate: We apply randomness to a proportion of the pipelines. This hyperparameter sets that proportion (between 0 and 1). crossover_rate: In each iteration we crossover or breed together some of our models to find similar ones. This sets the proportion of pipelines that we do this to. scoring: the objective function to determine the strongest models or offspring for example, accuracy. And finally cv, the cross-validation strategy to use which will be familiar from classic machine learning modeling you have done.\r\n",
        "\r\n",
        "7. A simple example\r\n",
        "Here we have a super simple example. You will notice the similarities to they way you have built models with Scikit Learn. Firstly creating the estimator with the hyperparameters. Here the TPOTClassifier is the estimator. Then using the dot-fit method and a scoring method, just like in Scikit Learn. We will keep default values for mutation_rate and crossover_rate as they are best left to the default without deeper knowledge on genetic programming. The verbosity parameter will print out the process as it goes. Notice how we are not even selecting algorithms or hyperparameters? TPOT does it all!\r\n",
        "\r\n",
        "8. Let's practice!\r\n",
        "Lets practice building our own genetic hyperparameter optimizer with TPOT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sjX72f9RH08"
      },
      "source": [
        "# Genetic Hyperparameter Tuning with TPOT\r\n",
        "\r\n",
        "You're going to undertake a simple example of genetic hyperparameter tuning. TPOT is a very powerful library that has a lot of features. You're just scratching the surface in this lesson, but you are highly encouraged to explore in your own time.\r\n",
        "\r\n",
        "This is a very small example. In real life, TPOT is designed to be run for many hours to find the best model. You would have a much larger population and offspring size as well as hundreds more generations to find a good model.\r\n",
        "\r\n",
        "You will create the estimator, fit the estimator to the training data and then score this on the test data.\r\n",
        "\r\n",
        "For this example we wish to use:\r\n",
        "\r\n",
        " - 3 generations\r\n",
        " - 4 in the population size\r\n",
        " - 3 offspring in each generation\r\n",
        " - accuracy for scoring\r\n",
        "\r\n",
        "A random_state of 2 has been set for consistency of results.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Assign the values outlined in the context to the inputs for tpot_clf.\r\n",
        "\r\n",
        "2. Create the tpot_clf classifier with the correct inputs.\r\n",
        "\r\n",
        "3. Fit the classifier to the training data (X_train & y_train are available in your workspace).\r\n",
        "\r\n",
        "4. Use the fitted classifier to score on the test set (X_test & y_test are available in your workspace)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siCwHP-mRgqZ"
      },
      "source": [
        "# Analysing TPOT's stability\r\n",
        "\r\n",
        "You will now see the random nature of TPOT by constructing the classifier with different random states and seeing what model is found to be best by the algorithm. This assists to see that TPOT is quite unstable when not run for a reasonable amount of time.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create the TPOT classifier, fit to the data and score using a random_state of 42.\r\n",
        "\r\n",
        "2. Now try using a random_state of 122. The numbers don't mean anything special, but should produce different results.\r\n",
        "\r\n",
        "3. Finally try using the random_state of 99. See how there is a different result again?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26amP399RxcK"
      },
      "source": [
        "# Congratulations!\r\n",
        "\r\n",
        "1. Wrap up\r\n",
        "Well done on reaching the end of the course! Let's summarize all your learnings.\r\n",
        "\r\n",
        "2. Hyperparameters vs Parameters\r\n",
        "You firstly learned the difference between hyperparameters and parameters. Remember that hyperparameters are inputs to the estimator or algorithm that you set, they are not learned by the algorithm in the modeling process. Parameters are learned by the algorithm and returned to you. You do not set these.\r\n",
        "\r\n",
        "3. Which hyperparameters & values?\r\n",
        "A few times during the course you explored best practice for setting hyperparameters and their values to search over. You learned some top tips like: Which hyperparameters are better to start with than others,for example with a Random Forest use case. That there are potentially silly values you can set for hyperparameters that will waste your effort. You need to beware of conflicting hyperparameter values, especially when the error may not be obvious and finally that this best practice is specific to each algorithm and hyperparameter. So you have some work to do researching and learning this!\r\n",
        "\r\n",
        "4. Remembering Grid Search\r\n",
        "We then learned about grid search. This is where we construct a grid of all the values we wish to test for all the different hyperparameters Then we undertake an exhaustive search of all the different combinations and finally pick the best model. You learned that this is a computationally expensive method but it is guaranteed to find the best model in the grid you specify. You can remember being reminded often the importance of setting good grid values!\r\n",
        "\r\n",
        "5. Remembering Random Search\r\n",
        "Our next method was random search. This was very similar to grid search The main difference was instead of trying every square (or hyperparameter combination) on the grid, we randomly selected a certain number. This method is more efficient at finding a reasonably good model faster but it is not guaranteed to find the best on your grid.\r\n",
        "\r\n",
        "6. From uninformed to informed search\r\n",
        "Finally we looked at some advanced methods that are known as 'informed search'. This is where each iteration learns from the last, as opposed to grid and random where you do all your modeling at once and then pick the best. These methods were: 'Coarse to Fine' where you iteratively build random searches to narrow your search space before a final grid search. An introduction to Bayesian hyperparameter tuning where you use the method of updating your prior beliefs when new evidence arrives about model performance. And finally genetic algorithms, drawing from nature and how evolution selects the best species as you select your best models over the generations.\r\n",
        "\r\n",
        "7. Thank you!\r\n",
        "Thanks for taking this course. I hope you learned some useful methodologies for your future work undertaking hyperparameter tuning in Python!"
      ]
    }
  ]
}