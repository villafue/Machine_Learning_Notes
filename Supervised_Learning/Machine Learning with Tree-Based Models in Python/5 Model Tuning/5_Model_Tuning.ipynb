{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5 Model Tuning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyQK7BqMEgcI+uf5NUj68z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Supervised_Learning/Machine%20Learning%20with%20Tree-Based%20Models%20in%20Python/5%20Model%20Tuning/5_Model_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08BBvnBx-Irp"
      },
      "source": [
        "# Model Tuning\r\n",
        "\r\n",
        "The hyperparameters of a machine learning model are parameters that are not learned from data. They should be set prior to fitting the model to the training set. In this chapter, you'll learn how to tune the hyperparameters of a tree-based model using grid search cross validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOlQjyrY-e4e"
      },
      "source": [
        "# Tuning a CART's Hyperparameters\r\n",
        "\r\n",
        "1. Tuning a CART's hyperparameters\r\n",
        "To obtain a better performance, the hyperparameters of a machine learning should be tuned.\r\n",
        "\r\n",
        "2. Hyperparameters\r\n",
        "Machine learning models are characterized by parameters and hyperparameters. Parameters are learned from data through training; examples of parameters include the split-feature and the split-point of a node in a CART. Hyperparameters are not learned from data; they should be set prior to training. Examples of hyperparameters include the maximum-depth and the splitting-criterion of a CART.\r\n",
        "\r\n",
        "3. What is hyperparameter tuning?\r\n",
        "Hyperparameter tuning consists of searching for the set of optimal hyperparameters for the learning algorithm. The solution involves finding the set of optimal hyperparameters yielding an optimal model. The optimal model yields an optimal score. The score function measures the agreement between true labels and a model's predictions. In sklearn, it defaults to accuracy for classifiers and r-squared for regressors. A model's generalization performance is evaluated using cross-validation.\r\n",
        "\r\n",
        "4. Why tune hyperparameters?\r\n",
        "A legitimate question that you may ask is: why bother tuning hyperparameters? Well, in scikit-learn, a model's default hyperparameters are not optimal for all problems. Hyperparameters should be tuned to obtain the best model performance.\r\n",
        "\r\n",
        "5. Approaches to hyperparameter tuning\r\n",
        "Now there are many approaches for hyperparameter tuning including: grid-search, random-search, and so on. In this course, we'll only be exploring the method of grid-search.\r\n",
        "\r\n",
        "6. Grid search cross validation\r\n",
        "In grid-search cross-validation, first you manually set a grid of discrete hyperparameter values. Then, you pick a metric for scoring model performance and you search exhaustively through the grid. For each set of hyperparameters, you evaluate each model's score. The optimal hyperparameters are those for which the model achieves the best cross-validation score. Note that grid-search suffers from the curse of dimensionality, i-dot-e-dot, the bigger the grid, the longer it takes to find the solution.\r\n",
        "\r\n",
        "7. Grid search cross validation: example\r\n",
        "Let's walk through a concrete example to understand this procedure. Consider the case of a CART where you search through the two-dimensional hyperparameter grid shown here. The dimensions correspond to the CART's maximum-depth and the minimum-percentage of samples per leaf. For each combination of hyperparameters, the cross-validation score is evaluated using k-fold CV for example. Finally, the optimal hyperparameters correspond to the model achieving the best cross-validation score.\r\n",
        "\r\n",
        "8. Inspecting the hyperparameters of a CART in sklearn\r\n",
        "Let's now see how we can inspect the hyperparameters of a CART in scikit-learn. You can first instantiate a DecisionTreeClassifier dt as shown here.\r\n",
        "\r\n",
        "9. Inspecting the hyperparameters of a CART in sklearn\r\n",
        "Then, call dt's -dot-get_params() method. This prints out a dictionary where the keys are the hyperparameter names. In the following, we'll only be optimizing max_depth, max_features and min_samples_leaf. Note that max_features is the number of features to consider when looking for the best split. When it's a float, it is interpreted as a percentage. You can learn more about these hyperparameters by consulting scikit-learn's documentation.\r\n",
        "\r\n",
        "10. Grid search CV in sklearn (Breast Cancer dataset)\r\n",
        "Let's now tune dt on the wisconsin breast cancer dataset which is already loaded and split into 80%-train and 20%-test. First, import GridSearchCV from sklearn-dot-model_selection. Then, define a dictionary called params_dt containing the names of the hyperparameters to tune as keys and lists of hyperparameter-values as values. Once done, instantiate a GridSearchCV object grid_dt by passing dt as an estimator and params_dt as param_grid. Also set scoring to accuracy and cv to 10 in order to use 10-fold stratified cross-validation for model evaluation. Finally, fit grid_dt to the training set.\r\n",
        "\r\n",
        "11. Extracting the best hyperparameters\r\n",
        "After training grid_dt, the best set of hyperparameter-values can be extracted from the attribute -dot-best_params_ of grid_dt. Also, the best cross validation accuracy can be accessed through grid_dt's -dot-best_score_ attribute.\r\n",
        "\r\n",
        "12. Extracting the best estimator\r\n",
        "Similarly, the best-model can be extracted using the -dot-best_estimator attribute. Note that this model is fitted on the whole training set because the refit parameter of GridSearchCV is set to True by default. Finally, you can evaluate this model's test set accuracy using the score method. The result is about 94-dot-7% while the score of an untuned CART is of 93%.\r\n",
        "\r\n",
        "13. Let's practice!\r\n",
        "Now it's your turn to practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwikLjG8-kdH"
      },
      "source": [
        "# Tree hyperparameters\r\n",
        "\r\n",
        "In the following exercises you'll revisit the Indian Liver Patient dataset which was introduced in a previous chapter.\r\n",
        "\r\n",
        "Your task is to tune the hyperparameters of a classification tree. Given that this dataset is imbalanced, you'll be using the ROC AUC score as a metric instead of accuracy.\r\n",
        "\r\n",
        "We have instantiated a DecisionTreeClassifier and assigned to dt with sklearn's default hyperparameters. You can inspect the hyperparameters of dt in your console.\r\n",
        "\r\n",
        "Which of the following is not a hyperparameter of dt?\r\n",
        "\r\n",
        "- min_impurity_decrease\r\n",
        " - Incorrect! min_impurity_decrease is a hyperparameter and it defaults to 0.\r\n",
        "\r\n",
        "- min_weight_fraction_leaf\r\n",
        " - Incorrect! min_weight_fraction_leaf is a hyperparameter and it defaults to 0.\r\n",
        "\r\n",
        "- min_features\r\n",
        " - Well done! There is no hyperparameter named min_features.\r\n",
        "- splitter\r\n",
        " - Incorrect! splitter is a hyperparameter and it defaults to 'best'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOCZNHtl_AcL"
      },
      "source": [
        "# Set the tree's hyperparameter grid\r\n",
        "In this exercise, you'll manually set the grid of hyperparameters that will be used to tune the classification tree dt and find the optimal classifier in the next exercise.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Define a grid of hyperparameters corresponding to a Python dictionary called params_dt with:\r\n",
        "\r\n",
        " -  the key 'max_depth' set to a list of values 2, 3, and 4\r\n",
        "\r\n",
        " - the key 'min_samples_leaf' set to a list of values 0.12, 0.14, 0.16, 0.18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOo2mwj9O9U"
      },
      "source": [
        "# Define params_dt\r\n",
        "params_dt = {\r\n",
        "    'max_depth': [2, 3, 4],\r\n",
        "    'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8wFEXqE_qW8"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great! Next comes performing the grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CkX6jJa_xdT"
      },
      "source": [
        "# Search for the optimal tree\r\n",
        "\r\n",
        "In this exercise, you'll perform grid search using 5-fold cross validation to find dt's optimal hyperparameters. Note that because grid search is an exhaustive process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:\r\n",
        "\r\n",
        "`grid_object.fit(X_train, y_train)`\r\n",
        "\r\n",
        "An untuned classification tree dt as well as the dictionary params_dt that you defined in the previous exercise are available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import GridSearchCV from sklearn.model_selection.\r\n",
        "\r\n",
        "2. Instantiate a GridSearchCV object using 5-fold CV by setting the parameters:\r\n",
        "\r\n",
        " - estimator to dt, param_grid to params_dt and\r\n",
        "\r\n",
        " - scoring to 'roc_auc'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MPWRAGZANrs"
      },
      "source": [
        "# Import GridSearchCV\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# Instantiate grid_dt\r\n",
        "grid_dt = GridSearchCV(estimator=dt,\r\n",
        "                       param_grid=params_dt,\r\n",
        "                       scoring='roc_auc',\r\n",
        "                       cv=5,\r\n",
        "                       n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd3PxRxcAOnF"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Awesome! As we said earlier, we will fit the model to the training data for you and in the next exercise you will compute the test set ROC AUC score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd6aTBnIARXl"
      },
      "source": [
        "# Evaluate the optimal tree\r\n",
        "\r\n",
        "In this exercise, you'll evaluate the test set ROC AUC score of grid_dt's optimal model.\r\n",
        "\r\n",
        "In order to do so, you will first determine the probability of obtaining the positive label for each test set observation. You can use the methodpredict_proba() of an sklearn classifier to compute a 2D array containing the probabilities of the negative and positive class-labels respectively along columns.\r\n",
        "\r\n",
        "The dataset is already loaded and processed for you (numerical features are standardized); it is split into 80% train and 20% test. X_test, y_test are available in your workspace. In addition, we have also loaded the trained GridSearchCV object grid_dt that you instantiated in the previous exercise. Note that grid_dt was trained as follows:\r\n",
        "\r\n",
        "`grid_dt.fit(X_train, y_train)`\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import roc_auc_score from sklearn.metrics.\r\n",
        "\r\n",
        "2. Extract the .best_estimator_ attribute from grid_dt and assign it to best_model.\r\n",
        "\r\n",
        "3. Predict the test set probabilities of obtaining the positive class y_pred_proba.\r\n",
        "\r\n",
        "4. Compute the test set ROC AUC score test_roc_auc of best_model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5ImqFidDIpC"
      },
      "source": [
        "# Import roc_auc_score from sklearn.metrics\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "# Extract the best estimator\r\n",
        "best_model = grid_dt.best_estimator_\r\n",
        "\r\n",
        "# Predict the test set probabilities of the positive class\r\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:,1]\r\n",
        "\r\n",
        "# Compute test_roc_auc\r\n",
        "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\r\n",
        "\r\n",
        "# Print test_roc_auc\r\n",
        "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Test set ROC AUC score: 0.610\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXzxxUvODOGL"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Great work! An untuned classification-tree would achieve a ROC AUC score of 0.54!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vW46tyDUtG"
      },
      "source": [
        "# Tuning a RF's Hyperparameters\r\n",
        "\r\n",
        "1. Tuning an RF's Hyperparameters\r\n",
        "Let's now turn to a case where we tune the hyperparameters of Random Forests which is an ensemble method.\r\n",
        "\r\n",
        "2. Random Forests Hyperparameters\r\n",
        "In addition to the hyperparameters of the CARTs forming random forests, the ensemble itself is characterized by other hyperparameters such as the number of estimators, whether it uses bootstraping or not and so on.\r\n",
        "\r\n",
        "3. Tuning is expensive\r\n",
        "As a note, hyperparameter tuning is computationally expensive and may sometimes lead only to very slight improvement of a model's performance. For this reason, it is desired to weigh the impact of tuning on the pipeline of your data analysis project as a whole in order to understand if it is worth pursuing.\r\n",
        "\r\n",
        "4. Inspecting RF Hyperparameters in sklearn\r\n",
        "To inspect the hyperparameters of a RandomForestRegressor, first, import RandomForestRegressor from sklearn.ensemble and then instantiate a RandomForestRegressor rf as shown here.\r\n",
        "\r\n",
        "5. Inspecting RF Hyperparameters in sklearn\r\n",
        "The hyperparameters of rf along with their default values can be accessed by calling rf's dot-get_params() method. In the following, we'll be optimizing n_estimators, max_depth, min_samples_leaf and max_features. You can learn more about these hyperparameters by consulting scikit-learn's documentation.\r\n",
        "\r\n",
        "6. GridSearchCV in sklearn (auto dataset)\r\n",
        "We'll perform grid-search cross-validation on the auto-dataset which is already loaded and split into 80%-train and 20%-test. First import mean_squared_error as MSE from sklearn.metrics and GridSearchCV from sklearn.model_selection. Then, define a dictionary called params_rf containing the grid of hyperparameters. Finally, instantiate a GridSearchCV object called grid_rf and pass the parameters rf as estimator, params_rf as param_grid. Also set cv to 3 to perform 3-fold cross-validation. In addition, set scoring to neg_mean_squared_error in order to use negative mean squared error as a metric. Note that the parameter verbose controls verbosity; the higher its value, the more messages are printed during fitting.\r\n",
        "\r\n",
        "7. Searching for the best hyperparameters\r\n",
        "You can now fit grid_rf to the training set as shown here. The output shows messages related to grid fitting as well as the obtained optimal model.\r\n",
        "\r\n",
        "8. Extracting the best hyperparameters\r\n",
        "You can extract rf's best hyperparameters by getting the attribute best_params_ from grid_rf. The results are shown here.\r\n",
        "\r\n",
        "9. Evaluating the best model performance\r\n",
        "You can also extract the best model from rf. This enables you to predict the test set labels and evaluate the test-set RMSE. The output shows a result of 3-dot-89. If you would have trained an untuned model, the RMSE would be 3-dot-98.\r\n",
        "\r\n",
        "10. Let's practice!\r\n",
        "Now let's try some examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty7N0eUhIpxr"
      },
      "source": [
        "# Random forests hyperparameters\r\n",
        "\r\n",
        "In the following exercises, you'll be revisiting the Bike Sharing Demand dataset that was introduced in a previous chapter. Recall that your task is to predict the bike rental demand using historical weather data from the Capital Bikeshare program in Washington, D.C.. For this purpose, you'll be tuning the hyperparameters of a Random Forests regressor.\r\n",
        "\r\n",
        "We have instantiated a RandomForestRegressor called rf using sklearn's default hyperparameters. You can inspect the hyperparameters of rf in your console.\r\n",
        "\r\n",
        "Which of the following is not a hyperparameter of rf?\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "- min_weight_fraction_leaf\r\n",
        " - Incorrect! min_weight_fraction_leaf is a hyperparameter and it defaults to 0.\r\n",
        "\r\n",
        "- criterion\r\n",
        " - Incorrect! criterion is a hyperparameter and it defaults to 'mse'.\r\n",
        "\r\n",
        "- learning_rate\r\n",
        " - Well done! There is no hyperparameter named learning_rate.\r\n",
        "\r\n",
        "- warm_start\r\n",
        " - Incorrect! warm_start is a hyperparameter and it defaults to False."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IGJ_E2I_Qa"
      },
      "source": [
        "# Set the hyperparameter grid of RF\r\n",
        "\r\n",
        "In this exercise, you'll manually set the grid of hyperparameters that will be used to tune rf's hyperparameters and find the optimal regressor. For this purpose, you will be constructing a grid of hyperparameters and tune the number of estimators, the maximum number of features used when splitting each node and the minimum number of samples (or fraction) per leaf.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Define a grid of hyperparameters corresponding to a Python dictionary called params_rf with:\r\n",
        "\r\n",
        " - the key 'n_estimators' set to a list of values 100, 350, 500\r\n",
        "\r\n",
        " - the key 'max_features' set to a list of values 'log2', 'auto', 'sqrt'\r\n",
        "\r\n",
        " - the key 'min_samples_leaf' set to a list of values 2, 10, 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm7aycJLJaYg"
      },
      "source": [
        "# Define the dictionary 'params_rf'\r\n",
        "params_rf = {\r\n",
        "    'n_estimators': [100, 350, 500],\r\n",
        "    'max_features': ['log2', 'auto', 'sqrt'],\r\n",
        "    'min_samples_leaf': [2, 10, 30]\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg2tcGUJbeu"
      },
      "source": [
        "# Search for the optimal forest\r\n",
        "\r\n",
        "In this exercise, you'll perform grid search using 3-fold cross validation to find rf's optimal hyperparameters. To evaluate each model in the grid, you'll be using the negative mean squared error metric.\r\n",
        "\r\n",
        "Note that because grid search is an exhaustive search process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:\r\n",
        "\r\n",
        "`grid_object.fit(X_train, y_train)`\r\n",
        "\r\n",
        "The untuned random forests regressor model rf as well as the dictionary params_rf that you defined in the previous exercise are available in your workspace.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import GridSearchCV from sklearn.model_selection.\r\n",
        "\r\n",
        "2. Instantiate a GridSearchCV object using 3-fold CV by using negative mean squared error as the scoring metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvoLjvp2Jpwi"
      },
      "source": [
        "# Import GridSearchCV\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "# Instantiate grid_rf\r\n",
        "grid_rf = GridSearchCV(estimator=rf,\r\n",
        "                       param_grid=params_rf,\r\n",
        "                       scoring='neg_mean_squared_error',\r\n",
        "                       cv=3,\r\n",
        "                       verbose=1,\r\n",
        "                       n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEiHxcayKDMc"
      },
      "source": [
        "# Evaluate the optimal forest\r\n",
        "\r\n",
        "In this last exercise of the course, you'll evaluate the test set RMSE of grid_rf's optimal model.\r\n",
        "\r\n",
        "The dataset is already loaded and processed for you and is split into 80% train and 20% test. In your environment are available X_test, y_test and the function mean_squared_error from sklearn.metrics under the alias MSE. In addition, we have also loaded the trained GridSearchCV object grid_rf that you instantiated in the previous exercise. Note that grid_rf was trained as follows:\r\n",
        "\r\n",
        "`grid_rf.fit(X_train, y_train)`\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Import mean_squared_error as MSE from sklearn.metrics.\r\n",
        "\r\n",
        "2. Extract the best estimator from grid_rf and assign it to best_model.\r\n",
        "\r\n",
        "3. Predict best_model's test set labels and assign the result to y_pred.\r\n",
        "\r\n",
        "4. Compute best_model's test set RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb1sImY1LuQV"
      },
      "source": [
        "# Import mean_squared_error from sklearn.metrics as MSE \r\n",
        "from sklearn.metrics import mean_squared_error as MSE\r\n",
        "\r\n",
        "# Extract the best estimator\r\n",
        "best_model = grid_rf.best_estimator_\r\n",
        "\r\n",
        "# Predict test set labels\r\n",
        "y_pred = best_model.predict(X_test)\r\n",
        "\r\n",
        "# Compute rmse_test\r\n",
        "rmse_test = MSE(y_test, y_pred)**(1/2)\r\n",
        "\r\n",
        "# Print rmse_test\r\n",
        "print('Test RMSE of best model: {:.3f}'.format(rmse_test))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Test RMSE of best model: 50.569\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBA4xR1FL3OP"
      },
      "source": [
        "# Congratulations!\r\n",
        "\r\n",
        "1. Congratulations!\r\n",
        "Congratulations on completing this course!\r\n",
        "\r\n",
        "2. How far you have come\r\n",
        "Take a moment to take a look at how far you have come! In chapter 1, you started off by understanding and applying the CART algorithm to train decision trees or CARTs for problems involving classification and regression. In chapter 2, you understood what the generalization error of a supervised learning model is. In addition, you also learned how underfitting and overfitting can be diagnosed with cross-validation. Furthermore, you learned how model ensembling can produce results that are more robust than individual decision trees. In chapter 3, you applied randomization through bootstrapping and constructed a diverse set of trees in an ensemble through bagging. You also explored how random forests introduces further randomization by sampling features at the level of each node in each tree forming the ensemble. Chapter 4 introduced you to boosting, an ensemble method in which predictors are trained sequentially and where each predictor tries to correct the errors made by its predecessor. Specifically, you saw how AdaBoost involved tweaking the weights of the training samples while gradient boosting involved fitting each tree using the residuals of its predecessor as labels. You also learned how subsampling instances and features can lead to a better performance through Stochastic Gradient Boosting. Finally, in chapter 5, you explored hyperparameter tuning through Grid Search cross-validation and you learned how important it is to get the most out of your models.\r\n",
        "\r\n",
        "3. Thank you!\r\n",
        "I hope you enjoyed taking this course as much as I enjoyed developing it. Finally, I encourage you to apply the skills you learned by practicing on real-world datasets.\r\n",
        "\r\n"
      ]
    }
  ]
}