{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression with XGBoost.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuixku5zw4P+fyBocm2AQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Supervised_Learning/Supervised%20Learning%20with%20Scikit-Learn/Extreme%20Gradient%20Boosting%20with%20XGBoost/02%20Regression%20with%20XGBoost/02%20Regression_with_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvn5Hk609LKu"
      },
      "source": [
        "# Regression with XGBoost\r\n",
        "\r\n",
        "After a brief review of supervised regression, you'll apply XGBoost to the regression task of predicting house prices in Ames, Iowa. You'll learn about the two kinds of base learners that XGboost can use as its weak learners, and review how to evaluate the quality of your regression models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmGQQsKx9RbH"
      },
      "source": [
        "# Regression review\r\n",
        "\r\n",
        "1. Regression review\r\n",
        "Congratulations on finishing chapter 1! Now that you've learned how to use XGBoost for classification, you'll learn how to use XGBoost for regression in this chapter.\r\n",
        "\r\n",
        "2. Regression basics\r\n",
        "Regression problems involve predicting continuous, or real, values. For example, if you're attempting to predict the height in centimeters a given person will be at 30 given some of their physical attributes at birth, you're solving a regression problem. Evaluating the quality of a regression model involves using a different set of metrics than those we described for use in classification problems in chapter 1.\r\n",
        "\r\n",
        "3. Common regression metrics\r\n",
        "In most cases, we use root mean squared error (RMSE) or the mean absolute error (MAE) to evaluate the quality of a regression model.\r\n",
        "\r\n",
        "4. Computing RMSE\r\n",
        "RMSE is computed by\r\n",
        "\r\n",
        "5. Computing RMSE\r\n",
        "taking the difference between the actual and the predicted values for what you are trying to predict,\r\n",
        "\r\n",
        "6. Computing RMSE\r\n",
        "squaring those differences, computing their mean, and taking that value's square root. This allows us to treat negative and positive differences equally, but tends to punish larger differences between predicted and actual values much more than smaller ones. MAE, on the other hand,\r\n",
        "\r\n",
        "7. Computing MAE\r\n",
        "simply sums the absolute differences between predicted and actual values across all of the samples we build our model on. Although MAE isn't affected by large differences as much as RMSE, it lacks some nice mathematical properties that make it much less frequently used as an evaluation metric.\r\n",
        "\r\n",
        "8. Common regression algorithms\r\n",
        "Some common algorithms that are used for regression problems include linear regression and decision trees. It's important to briefly note here that some algorithms,\r\n",
        "\r\n",
        "9. Algorithms for both regression and classification\r\n",
        "such as decision trees, can be used for both regression as well as classification tasks, which, as we will see, is one of their important properties that makes them prime candidates to be building blocks for XGBoost models.\r\n",
        "\r\n",
        "1 https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/ com.ibm.spss.modeler.help/nodes_treebuilding.htm\r\n",
        "10. Let's practice!\r\n",
        "Awesome, let's test your regression knowledge with a multiple choice questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA75-NUPJQcD"
      },
      "source": [
        "# Which of these is a regression problem?\n",
        "Here are 4 potential machine learning problems you might encounter in the wild. Pick the one that is a clear example of a regression problem.\n",
        "\n",
        "**Incorrect Answers**\n",
        "\n",
        "1. Recommending a restaurant to a user given their past history of restaurant visits and reviews for a dining aggregator app.\n",
        " - This is a recommendation problem.\n",
        "\n",
        "2. Predicting which of several thousand diseases a given person is most likely to have given their symptoms.\n",
        " - This is a multi-class classification problem.\n",
        "\n",
        "3. Tagging an email as spam/not spam based on its content and metadata (sender, time sent, etc.).\n",
        " - This is a binary classification problem.\n",
        "**Correct Answer**\n",
        "\n",
        "Predicting the expected payout of an auto insurance claim given claim properties (car, accident type, driver prior history, etc.).\n",
        " - Well done! This is indeed an example of a regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEMooPPx9ZmS"
      },
      "source": [
        "# Objective (loss) functions and base learners\r\n",
        "\r\n",
        "1. Objective (loss) functions and base learners\r\n",
        "Let's talk a bit about objective functions and base learners so we can develop better intuitions about both concepts, as they are critical to understand in order for you to be able to grasp why XGBoost is such a powerful approach to building supervised regression models.\r\n",
        "\r\n",
        "2. Objective Functions and Why We Use Them\r\n",
        "An objective or loss function quantifies how far off our prediction is from the actual result for a given data point. It maps the difference between the prediction and the target to a real number. When we construct any machine learning model, we do so in the hopes that it minimizes the loss function across all of the data points we pass in. That's our ultimate goal, the smallest possible loss.\r\n",
        "\r\n",
        "3. Common loss functions and XGBoost\r\n",
        "Loss functions have specific naming conventions in XGBoost. For regression models, the most common loss function used is called reg linear. For binary classification models, the most common loss functions used are reg logistic, when you simply want the category of the target, and binary logistic, when you want the actual predicted probability of the positive class. So, in chapter 1, we were implicitly using the reg logistic loss function when building our classification models in XGBoost.\r\n",
        "\r\n",
        "4. Base learners and why we need them\r\n",
        "As mentioned before, XGBoost is an ensemble learning method composed of many individual models that are added together to generate a single prediction. Each of the individual models that are trained and combined are called base learners. The goal of XGBoost is to have base learners that is slightly better than random guessing on certain subsets of training examples, and uniformly bad at the remainder, so that when all of the predictions are combined, the uniformly bad predictions cancel out and those slightly better than chance combine into a single very good prediction. Let's look at a couple examples using trees and linear base learners in XGBoost.\r\n",
        "\r\n",
        "5. Trees as base learners example: Scikit-learn API\r\n",
        "Here's an example of how to train an XGBoost regression model with trees as base learners using XGBoost's scikit-learn compatible API. We will use the Boston Housing dataset from UCI's machine learning repository as an example. In lines 1-5 we import the libraries we need and load in the data. In lines 6 and 7, we convert our data into our X matrix and y vector and split into training and test sets as we've done before. In lines 8-10, we create our XGBoost regressor object, this time making sure we use the reg linear objective function, fit it to our training data, and generate our predictions on the test set.\r\n",
        "\r\n",
        "6. Trees as base learners example: Scikit-learn API\r\n",
        "And finally in lines 11 and 12 we compute the RMSE and print the result to screen.\r\n",
        "\r\n",
        "7. Linear base learners example: learning API only\r\n",
        "To use linear base learners, we have to use the learning API in XGBoost. Here's an example. In lines 1-7 we do what we did as before, loading in appropriate libraries and data. In lines 8 and 9 we convert our training and testing sets into DMatrix objects, as is required by the learning API. In line 10 we create a parameter dictionary explicitly specifying the base learner we want as gblinear, and the reg linear objective function we want to use. In lines 11-12 we train our model on the training set and generate predictions using the test set.\r\n",
        "\r\n",
        "8. Linear base learners example: learning API only\r\n",
        "In lines 13 and 14, we compute our rmse and print to screen, as we did before.\r\n",
        "\r\n",
        "9. Let's get to work!\r\n",
        "Ok, lets get to work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saGMK_CoKOgw"
      },
      "source": [
        "# Decision trees as base learners\n",
        "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
        "\n",
        "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\".\n",
        "\n",
        "xgboost has been imported as xgb and the arrays for the features and the target are available in X and y, respectively.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- Split df into training and testing sets, holding out 20% for testing. Use a random_state of 123.\n",
        "- Instantiate the XGBRegressor as xg_reg, using a seed of 123. Specify an objective of \"reg:linear\" and use 10 trees. Note: You don't have to specify booster=\"gbtree\" as this is the default.\n",
        "- Fit xg_reg to the training data and predict the labels of the test set. Save the predictions in a variable called preds.\n",
        "- Compute the rmse using np.sqrt() and the mean_squared_error() function from sklearn.metrics, which has been pre-imported.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Well done! Next, you'll train an XGBoost model using linear base learners and XGBoost's learning API. Will it perform better or worse?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7fwFQZRJJaW"
      },
      "source": [
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBRegressor: xg_reg\n",
        "xg_reg = xgb.XGBRegressor(objective='reg:linear', n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the regressor to the training set\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Compute the rmse: rmse\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"RMSE: %f\" % (rmse))\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [00:27:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    RMSE: 78847.401758\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QloXa2E9QUwk"
      },
      "source": [
        "# Linear base learners\n",
        "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
        "\n",
        "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
        "\n",
        "Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past.\n",
        "\n",
        "Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API.\n",
        "\n",
        "Instructions\n",
        "\n",
        "- Create two DMatrix objects - DM_train for the training set (X_train and y_train), and DM_test (X_test and y_test) for the test set.\n",
        "\n",
        "- Create a parameter dictionary that defines the \"booster\" type you will use (\"gblinear\") as well as the \"objective\" you will minimize (\"reg:linear\").\n",
        "\n",
        "- Train the model using xgb.train(). You have to specify arguments for the following parameters: params, dtrain, and num_boost_round. Use 5 boosting rounds.\n",
        "\n",
        "- Predict the labels on the test set using xg_reg.predict(), passing it DM_test. Assign to preds.\n",
        "\n",
        "- Hit 'Submit Answer' to view the RMSE!\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Interesting - it looks like linear base learners performed better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmGXUFuZU4Ch"
      },
      "source": [
        "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
        "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
        "DM_test =  xgb.DMatrix(data=X_test, label=y_test)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(DM_test)\n",
        "\n",
        "# Compute and print the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [00:47:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    RMSE: 41929.341664\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ibQMz4AUyGP"
      },
      "source": [
        "# Evaluating model quality\n",
        "It's now time to begin evaluating model quality.\n",
        "\n",
        "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame df.\n",
        "\n",
        "**Instructions 1/2**\n",
        "\n",
        "- Perform 4-fold cross-validation with 5 boosting rounds and \"rmse\" as the metric.\n",
        "\n",
        "- Extract and print the final boosting round RMSE.\n",
        "\n",
        "**Instructions 2/2**\n",
        "\n",
        "- Perform 4-fold cross-validation with 5 boosting rounds and \"rmse\" as the metric.\n",
        "\n",
        "- Extract and print the final boosting round RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzCMfdNOVxtl"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final boosting round metric\n",
        "print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [00:56:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [00:56:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [00:56:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [00:56:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "       train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
        "    0    141767.527344      429.450237   142980.429688    1193.794436\n",
        "    1    102832.542969      322.473304   104891.396485    1223.155480\n",
        "    2     75872.617187      266.469946    79478.939453    1601.341376\n",
        "    3     57245.650390      273.626583    62411.920899    2220.150028\n",
        "    4     44401.296875      316.423413    51348.280274    2963.379319\n",
        "    4    51348.280274\n",
        "    Name: test-rmse-mean, dtype: float64\n",
        "'''\n",
        "\n",
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final boosting round metric\n",
        "print((cv_results[\"test-mae-mean\"]).tail(1))\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [01:00:55] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [01:00:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [01:00:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [01:00:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "       train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
        "    0   127343.484375     668.335266  127633.982422   2404.005021\n",
        "    1    89770.050782     456.959206   90122.503906   2107.912235\n",
        "    2    63580.791992     263.407499   64278.563477   1887.565119\n",
        "    3    45633.153320     151.884919   46819.166015   1459.819399\n",
        "    4    33587.092774      86.999100   35670.645508   1140.606558\n",
        "    4    35670.645508\n",
        "    Name: test-mae-mean, dtype: float64\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDfmX_0eGbMo"
      },
      "source": [
        "# Using regularization in XGBoost\n",
        "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset.\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- Create your DMatrix from X and y as before.\n",
        "\n",
        "- Create an initial parameter dictionary specifying an \"objective\" of \"reg:linear\" and \"max_depth\" of 3.\n",
        "\n",
        "- Use xgb.cv() inside of a for loop and systematically vary the \"lambda\" value by passing in the current l2 value (reg).\n",
        "\n",
        "- Append the \"test-rmse-mean\" from the last boosting round for each cross-validated xgboost model.\n",
        "\n",
        "- Hit 'Submit Answer' to view the results. What do you notice?\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Nice work! It looks like as as the value of 'lambda' increases, so does the RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yihexw6bH_If"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "reg_params = [1, 10, 100]\n",
        "\n",
        "# Create the initial parameter dictionary for varying l2 strength: params\n",
        "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
        "\n",
        "# Create an empty list for storing rmses as a function of l2 complexity\n",
        "rmses_l2 = []\n",
        "\n",
        "# Iterate over reg_params\n",
        "for reg in reg_params:\n",
        "\n",
        "    # Update l2 strength\n",
        "    params[\"lambda\"] = reg\n",
        "    \n",
        "    # Pass this updated param dictionary into cv\n",
        "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append best rmse (final round) to rmses_l2\n",
        "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
        "\n",
        "# Look at best rmse per l2 param\n",
        "print(\"Best rmse as a function of l2:\")\n",
        "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [23:09:39] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [23:09:40] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [23:09:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [23:09:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [23:09:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    [23:09:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "    Best rmse as a function of l2:\n",
        "        l2          rmse\n",
        "    0    1  52275.359375\n",
        "    1   10  57746.064453\n",
        "    2  100  76624.625000\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nW4QwBeJU5D"
      },
      "source": [
        "# Visualizing individual XGBoost trees\n",
        "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
        "\n",
        "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.\n",
        "\n",
        "Instructions\n",
        "\n",
        "- Create a parameter dictionary with an \"objective\" of \"reg:linear\" and a \"max_depth\" of 2.\n",
        "\n",
        "- Train the model using 10 boosting rounds and the parameter dictionary you created. Save the result in xg_reg.\n",
        "\n",
        "- Plot the first tree using xgb.plot_tree(). It takes in two arguments - the model (in this case, xg_reg), and num_trees, which is 0-indexed. So to plot the first tree, specify num_trees=0.\n",
        "\n",
        "- Plot the fifth tree.\n",
        "\n",
        "- Plot the last (tenth) tree sideways. To do this, specify the additional keyword argument rankdir=\"LR\".\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "Excellent! Have a look at each of the plots. They provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions. This allows us to identify which features are the most important in determining house price. In the next exercise, you'll learn another way of visualizing feature importances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TMlJkyGKlU_"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the first tree\n",
        "xgb.plot_tree(xg_reg, num_trees=0)\n",
        "plt.show()\n",
        "\n",
        "# Plot the fifth tree\n",
        "xgb.plot_tree(xg_reg, num_trees=4)\n",
        "plt.show()\n",
        "\n",
        "# Plot the last tree sideways\n",
        "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "<script.py> output:\n",
        "    [23:20:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsiwmYOENhCG"
      },
      "source": [
        "# Visualizing feature importances: What features are most important in my dataset\n",
        "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
        "\n",
        "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!\n",
        "\n",
        "**Instructions**\n",
        "\n",
        "- Create your DMatrix from X and y as before.\n",
        "\n",
        "- Create a parameter dictionary with appropriate \"objective\" (\"reg:linear\") and a \"max_depth\" of 4.\n",
        "\n",
        "- Train the model with 10 boosting rounds, exactly as you did in the previous exercise.\n",
        "\n",
        "- Use xgb.plot_importance() and pass in the trained model to generate the graph of feature importances.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Brilliant! It looks like GrLivArea is the most important feature. Congratulations on completing Chapter 2!"
      ]
    }
  ]
}