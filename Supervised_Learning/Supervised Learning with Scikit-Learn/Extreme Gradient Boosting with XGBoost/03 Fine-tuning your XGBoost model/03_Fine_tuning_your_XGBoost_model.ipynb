{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 Fine-tuning your XGBoost model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNVAg40Tk5XRGo7As0vplqQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villafue/Machine_Learning_Notes/blob/master/Supervised_Learning/Supervised%20Learning%20with%20Scikit-Learn/Extreme%20Gradient%20Boosting%20with%20XGBoost/03%20Fine-tuning%20your%20XGBoost%20model/03_Fine_tuning_your_XGBoost_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT5VHYHsnjjE"
      },
      "source": [
        "# Fine-tuning your XGBoost model\r\n",
        "\r\n",
        "This chapter will teach you how to make your XGBoost models as performant as possible. You'll learn about the variety of parameters that can be adjusted to alter the behavior of XGBoost and how to tune them efficiently so that you can supercharge the performance of your models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CAYVWuhpYR5"
      },
      "source": [
        "# Why tune your model?\r\n",
        "\r\n",
        "1. Why tune your model?\r\n",
        "So far, you've learned how to use XGBoost to solve classification and regression problems. Now, you'll learn how to supercharge those models by tuning them. To motivate the reason behind this chapter on tuning your XGBoost model, let's just take a look at 2 cases, one where we take the simplest XGBoost model possible and compute a cross-validated RMSE, and then do the same exact thing with a tuned XGBoost model. What do you think the effect of model tuning on the overall reduction in RMSE will be?\r\n",
        "\r\n",
        "2. Untuned model example\r\n",
        "In lines 1-6, we simply load in the necessary libraries and ames housing data, and then convert our data into a DMatrix. In line 7, we create the most basic parameter configuration possible, only passing in the objective function we need to create a regression XGBoost model. This parameter configuration will be made much more complex as we tune our models. In fact, when performing parameter searches, we will use a dictionary that we typically call a parameter grid, because it will contain ranges of values over which we will search to find an optimal configuration. More on that later. In line 8, we run our cross-validation in XGBoost, passing in the simple parameter grid and telling it to run 4-fold cross validation, and to ouput the rmse as an evaluation metric. In line 9, we simply print the final rmse of the untuned model to screen, which is around 34600 dollars.\r\n",
        "\r\n",
        "3. Tuned model example\r\n",
        "Now let's take a look at a tuned example. Again, in lines 1-6, we load in the necessary libraries and ames housing data, and then convert our data into a DMatrix. In line 7, we create a more tuned parameter configuration, setting colsample_bytree, learning_rate, and max_depth to better values. These are a few of the more important xgboost parameters that can be tuned, and you will learn more about and practice tuning these parameters later in this chapter. In line 8, we run our cross-validation in XGBoost, passing in our tuned parameter grid, as well as setting the number of trees to be constructed at 200, and again running 4-fold cross validation, and outputting the rmse as an evaluation metric. In line 9, we print the final rmse of the tuned model to screen, which is around 29800 dollars. That's an almost 14% reduction in RMSE!\r\n",
        "\r\n",
        "4. Let's tune some models!\r\n",
        "Now that you see that you can get a significant improvement in model performance by tuning an XGBoost model, let's have you start doing some tuning yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77seol1esuQc"
      },
      "source": [
        "# When is tuning your model a bad idea?\r\n",
        "\r\n",
        "Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model?\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "1. You have lots of examples from some dataset and very many features at your disposal.\r\n",
        " - Sorry! If you have lots of examples and features, tuning your model is a great idea.\r\n",
        "\r\n",
        "2. You are very short on time before you must push an initial model to production and have little data to train your model on.\r\n",
        " - Yup! You cannot tune if you do not have time!\r\n",
        "\r\n",
        "You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.\r\n",
        " - This is a perfect example of when you should definitely tune your model.\r\n",
        "\r\n",
        "You must squeeze out every last bit of performance out of your xgboost model.\r\n",
        " - If you are trying to squeeze out performance, you are definitely going to tune your model. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cg6YBgwKQF4"
      },
      "source": [
        "# Tuning the number of boosting rounds\r\n",
        "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\r\n",
        "\r\n",
        "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a DMatrix called housing_dmatrix from X and y.\r\n",
        "\r\n",
        "2. Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:linear\") and \"max_depth\" (set it to 3).\r\n",
        "\r\n",
        "3. Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\r\n",
        "\r\n",
        "4. Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\r\n",
        "\r\n",
        "5. num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZGmQnn4WfuH"
      },
      "source": [
        "# Create the DMatrix: housing_dmatrix\r\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\r\n",
        "\r\n",
        "# Create the parameter dictionary for each tree: params \r\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\r\n",
        "\r\n",
        "# Create list of number of boosting rounds\r\n",
        "num_rounds = [5, 10, 15]\r\n",
        "\r\n",
        "# Empty list to store final round rmse per XGBoost model\r\n",
        "final_rmse_per_round = []\r\n",
        "\r\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\r\n",
        "for curr_num_rounds in num_rounds:\r\n",
        "\r\n",
        "    # Perform cross-validation: cv_results\r\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\r\n",
        "    \r\n",
        "    # Append final round RMSE\r\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n",
        "\r\n",
        "# Print the resultant DataFrame\r\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\r\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:07:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "       num_boosting_rounds          rmse\r\n",
        "    0                    5  50903.299479\r\n",
        "    1                   10  34774.191406\r\n",
        "    2                   15  32895.098307\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VovDObQfLY-v"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Awesome! As you can see, increasing the number of boosting rounds decreases the RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuN4YLghML0P"
      },
      "source": [
        "# Automated boosting round selection using early_stopping\r\n",
        "\r\n",
        "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\r\n",
        "\r\n",
        "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\r\n",
        "\r\n",
        "Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Perform 3-fold cross-validation with early stopping and \"rmse\" as your metric. Use 10 early stopping rounds and 50 boosting rounds. Specify a seed of 123 and make sure the output is a pandas DataFrame. Remember to specify the other parameters such as dtrain, params, and metrics.\r\n",
        "\r\n",
        "2. Print cv_results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ljbvtjFQNyB"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\r\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\r\n",
        "\r\n",
        "# Create the parameter dictionary for each tree: params\r\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\r\n",
        "\r\n",
        "# Perform cross-validation with early stopping: cv_results\r\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, metrics=\"rmse\", as_pandas=True, seed=123, early_stopping_rounds=10)\r\n",
        "\r\n",
        "# Print cv_results\r\n",
        "print(cv_results)\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [04:28:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:28:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:28:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "        train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\r\n",
        "    0     141871.635417      403.636200   142640.656250     705.559400\r\n",
        "    1     103057.033854       73.772531   104907.664062     111.112417\r\n",
        "    2      75975.966146      253.726099    79262.054687     563.764349\r\n",
        "    3      57420.531250      521.656754    61620.135417    1087.693857\r\n",
        "    4      44552.955729      544.170190    50437.562500    1846.448017\r\n",
        "    5      35763.947917      681.797248    43035.661458    2034.469207\r\n",
        "    6      29861.464193      769.571238    38600.880208    2169.796232\r\n",
        "    7      25994.675130      756.519115    36071.817708    2109.795430\r\n",
        "    8      23306.835937      759.237670    34383.184896    1934.546688\r\n",
        "    9      21459.769531      745.624998    33509.141276    1887.375284\r\n",
        "    10     20148.722005      749.611886    32916.808594    1850.894249\r\n",
        "    11     19215.382813      641.387565    32197.833333    1734.456784\r\n",
        "    12     18627.388021      716.257152    31770.852214    1802.155558\r\n",
        "    13     17960.694661      557.043073    31482.782552    1779.123767\r\n",
        "    14     17559.736979      631.412969    31389.991537    1892.319150\r\n",
        "    15     17205.712891      590.171393    31302.882162    1955.165902\r\n",
        "    16     16876.571940      703.631755    31234.058594    1880.705796\r\n",
        "    17     16597.662110      703.677609    31318.348308    1828.861504\r\n",
        "    18     16330.460937      607.274494    31323.633464    1775.909418\r\n",
        "    19     16005.973307      520.471073    31204.135417    1739.076156\r\n",
        "    20     15814.301107      518.604195    31089.862630    1756.021674\r\n",
        "    21     15493.405924      505.616658    31047.996094    1624.673955\r\n",
        "    22     15270.734049      502.019527    31056.916015    1668.042812\r\n",
        "    23     15086.381836      503.912899    31024.983724    1548.985354\r\n",
        "    24     14917.608399      486.206187    30983.684896    1663.130201\r\n",
        "    25     14709.589844      449.668898    30989.477865    1686.667141\r\n",
        "    26     14457.286458      376.787206    30952.114583    1613.172955\r\n",
        "    27     14185.567383      383.102234    31066.902344    1648.534310\r\n",
        "    28     13934.066732      473.465521    31095.640625    1709.225327\r\n",
        "    29     13749.645182      473.670437    31103.887370    1778.880069\r\n",
        "    30     13549.836589      454.898834    30976.085287    1744.514533\r\n",
        "    31     13413.484700      399.603166    30938.469401    1746.052597\r\n",
        "    32     13275.916016      415.408786    30931.001302    1772.468988\r\n",
        "    33     13085.878255      493.792509    30929.057291    1765.540568\r\n",
        "    34     12947.181641      517.790106    30890.630859    1786.510559\r\n",
        "    35     12846.027018      547.732021    30884.492839    1769.728719\r\n",
        "    36     12702.379232      505.523221    30833.542969    1691.003483\r\n",
        "    37     12532.244141      508.298241    30856.687500    1771.445978\r\n",
        "    38     12384.054687      536.224681    30818.016927    1782.784630\r\n",
        "    39     12198.444010      545.165197    30839.392578    1847.327022\r\n",
        "    40     12054.583333      508.841412    30776.966146    1912.780507\r\n",
        "    41     11897.036133      477.177991    30794.702474    1919.674832\r\n",
        "    42     11756.221354      502.992395    30780.955729    1906.819108\r\n",
        "    43     11618.846029      519.837502    30783.756511    1951.259784\r\n",
        "    44     11484.080078      578.428250    30776.731120    1953.446309\r\n",
        "    45     11356.553060      565.368380    30758.542969    1947.455873\r\n",
        "    46     11193.558268      552.298848    30729.970703    1985.699788\r\n",
        "    47     11071.315755      604.089526    30732.663411    1966.997809\r\n",
        "    48     10950.778646      574.862348    30712.241536    1957.751573\r\n",
        "    49     10824.865885      576.665756    30720.854167    1950.511057\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGHqFahnQar3"
      },
      "source": [
        "# Overview of XGBoost's hyperparameters\r\n",
        "\r\n",
        "1. Tunable parameters in XGBoost\r\n",
        "Let's now go over the differences in what parameters can be tuned for each kind of base model in XGBoost. The parameters that can be tuned are significantly different for each base learner.\r\n",
        "\r\n",
        "2. Common tree tunable parameters\r\n",
        "For the tree base learner, which is the one you should use in almost every single case, the most frequently tuned parameters are outlined below. The learning rate affects how quickly the model fits the residual error using additional base learners. A low learning rate will require more boosting rounds to achieve the same reduction in residual error as an XGBoost model with a high learning rate. Gamma, alpha, and lambda were described in chapter 2 and all have an effect on how strongly regularized the trained model will be. Max_depth must a positive integer value and affects how deeply each tree is allowed to grow during any given boosting round. Subsample must be a value between 0 and 1 and is the fraction of the total training set that can be used for any given boosting round. If the value is low, then the fraction of your training data used would per boosting round would be low and you may run into underfitting problems, a value that is very high can lead to overfitting as well. Colsample_bytree is the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. A large value means that almost all features can be used to build a tree during a given boosting round, whereas a small value means that the fraction of features that can be selected from is very small. In general, smaller colsample_bytree values can be thought of as providing additional regularization to the model, whereas using all columns may in certain cases overfit a trained model.\r\n",
        "\r\n",
        "3. Linear tunable parameters\r\n",
        "For the linear base learner, the number of tunable parameters is significantly smaller. You only have access to l1 and l2 regularization on the weights associated with any given feature, and then another regularization term that can be applied to the model's bias. Finally, its important to mention that the number of boosting rounds (that is, either the number of trees you build or the number of linear base learners you construct) is itself a tunable parameter.\r\n",
        "\r\n",
        "4. Let's get to some tuning!\r\n",
        "Now that we've covered the parameters that are usually tuned when using XGBoost, lets get to some tuning!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TruxaccNUIAe"
      },
      "source": [
        "# Tuning eta\r\n",
        "\r\n",
        "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\r\n",
        "\r\n",
        "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\r\n",
        "\r\n",
        "2. Iterate over your eta_vals list using a for loop.\r\n",
        "\r\n",
        "3. In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\r\n",
        "\r\n",
        "4. Append the final round RMSE to the best_rmse list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJYmmmh7Wh30"
      },
      "source": [
        "# Create your housing DMatrix: housing_dmatrix\r\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\r\n",
        "\r\n",
        "# Create the parameter dictionary for each tree (boosting round)\r\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\r\n",
        "\r\n",
        "# Create list of eta values and empty list to store final round rmse per xgboost model\r\n",
        "eta_vals = [0.001, 0.01, 0.1]\r\n",
        "best_rmse = []\r\n",
        "\r\n",
        "# Systematically vary the eta \r\n",
        "for curr_val in eta_vals:\r\n",
        "\r\n",
        "    params[\"eta\"] = curr_val\r\n",
        "    \r\n",
        "    # Perform cross-validation: cv_results\r\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, nfold=3, params=params, num_boost_round=10, metrics=\"rmse\", early_stopping_rounds=5, seed=123, as_pandas=True)\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    # Append the final round rmse to best_rmse\r\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n",
        "\r\n",
        "# Print the resultant DataFrame\r\n",
        "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [04:55:53] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "         eta      best_rmse\r\n",
        "    0  0.001  195736.406250\r\n",
        "    1  0.010  179932.187500\r\n",
        "    2  0.100   79759.414063\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEqFrWcLW4Ds"
      },
      "source": [
        "# Tuning max_depth\r\n",
        "\r\n",
        "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\r\n",
        "\r\n",
        "2. Iterate over your max_depths list using a for loop.\r\n",
        "\r\n",
        "3. Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyE2htOHXPVe"
      },
      "source": [
        "# Create your housing DMatrix\r\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\r\n",
        "\r\n",
        "# Create the parameter dictionary\r\n",
        "params = {\"objective\":\"reg:linear\"}\r\n",
        "\r\n",
        "# Create list of max_depth values\r\n",
        "max_depths = [2, 5, 10, 20]\r\n",
        "best_rmse = []\r\n",
        "\r\n",
        "# Systematically vary the max_depth\r\n",
        "for curr_val in max_depths:\r\n",
        "\r\n",
        "    params[\"max_depth\"] = curr_val\r\n",
        "    \r\n",
        "    # Perform cross-validation\r\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, nfold=2, params=params, num_boost_round=10, metrics=\"rmse\", early_stopping_rounds=5, seed=123, as_pandas=True)\r\n",
        "    \r\n",
        "\r\n",
        "    # Append the final round rmse to best_rmse\r\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n",
        "\r\n",
        "# Print the resultant DataFrame\r\n",
        "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:04:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "       max_depth     best_rmse\r\n",
        "    0          2  37957.468750\r\n",
        "    1          5  35596.599610\r\n",
        "    2         10  36065.548829\r\n",
        "    3         20  36739.578125\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOOvGAy0YtIZ"
      },
      "source": [
        "# Tuning colsample_bytree\r\n",
        "\r\n",
        "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\r\n",
        "\r\n",
        "2. Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfLJyfn6ZSij"
      },
      "source": [
        "# Create your housing DMatrix\r\n",
        "housing_dmatrix = xgb.DMatrix(data=X,label=y)\r\n",
        "\r\n",
        "# Create the parameter dictionary\r\n",
        "params={\"objective\":\"reg:linear\",\"max_depth\":3}\r\n",
        "\r\n",
        "# Create list of hyperparameter values: colsample_bytree_vals\r\n",
        "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\r\n",
        "best_rmse = []\r\n",
        "\r\n",
        "# Systematically vary the hyperparameter value \r\n",
        "for curr_val in colsample_bytree_vals:\r\n",
        "\r\n",
        "    params[\"colsample_bytree\"] = curr_val\r\n",
        "    \r\n",
        "    # Perform cross-validation\r\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\r\n",
        "                 num_boost_round=10, early_stopping_rounds=5,\r\n",
        "                 metrics=\"rmse\", as_pandas=True, seed=123)\r\n",
        "    \r\n",
        "    # Append the final round rmse to best_rmse\r\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\r\n",
        "\r\n",
        "# Print the resultant DataFrame\r\n",
        "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    [05:07:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [05:07:46] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "       colsample_bytree     best_rmse\r\n",
        "    0               0.1  48193.451172\r\n",
        "    1               0.5  36013.544922\r\n",
        "    2               0.8  35932.962891\r\n",
        "    3               1.0  35836.042969\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muntBKMSZXPG"
      },
      "source": [
        "Conclusion\r\n",
        "\r\n",
        "Awesome! There are several other individual parameters that you can tune, such as \"subsample\", which dictates the fraction of the training data that is used during any given boosting round. Next up: Grid Search and Random Search to tune XGBoost hyperparameters more efficiently!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accQXu_7Zbyq"
      },
      "source": [
        "# Review of grid search and random search\r\n",
        "\r\n",
        "1. Review of grid search and random search\r\n",
        "How do we find the optimal values for several hyperparameters simultaneously, leading to the lowest loss possible, when their values interact in in non-obvious, non-linear ways? Two common strategies for choosing several hyperparameter values simultaneously are Grid Search and Random Search, so it's important that we review them here, and see what their advantages and disadvantages are, by looking at some examples of how both can be used with the XGBoost and scikit-learn packages.\r\n",
        "\r\n",
        "2. Grid search: review\r\n",
        "Grid Search is a method of exhaustively searching through a collection of possible parameter values. For example, if you have 2 hyperparameters you would like to tune, and 4 possible values for each hyperparameter, then a grid search over that parameter space would try all 16 possible parameter configurations. In a grid search, you try every parameter configuration, evaluate some metric for that configuration, and pick the parameter configuration that gave you the best value for the metric you were using, which in our case will be the root mean squared error.\r\n",
        "\r\n",
        "3. Grid search: example\r\n",
        "Let's go over an example of how to grid search over several hyperparameters using XGBoost and scikit learn. In lines 1-4 we load in the necessary libraries, including GridSearchCV from sklearn dot model_selection. In lines 5-7 we load in our dataset and convert it into a DMatrix. In line 8 we create our grid of hyperparameters we want to search over. We selected 4 different learning rates (or eta values), 3 different subsample values, and a single number of trees. The total number of distinct hyperparameter configurations is 12, so 12 different models will be built. In line 9 we create our regressor, and then in line 10 we pass the xgbregressor object, parameter grid, evaluation metric, and number of cross validation folds to GridSearchCV and then immediately fit that gridsearch object in line 11, just like every other scikit learn estimator object we've done this to in the past. In line 12, having fit the gridsearch object, we can extract the best parameters the grid search found, and print them to the screen. In line 13, we get the RMSE that corresponds to the best parameters found, and see that it's ~28500 dollars.\r\n",
        "\r\n",
        "4. Random search: review\r\n",
        "Random search is significantly different from grid search in that the number of models that you are required to iterate over doesn't grow as you expand the overall hyperparameter space. In random search, you get to decide how many models, or iterations, you want to try out before stopping. Random search simply involves drawing a random combination of possible hyperparameter values from the range of allowable hyperparameters a set number of times. Each time, you train a model with the selected hyperparameters, evaluate the performance of that model, and then rinse and repeat. When you've created the number of models you had specified initially, you simply pick the best one. To finish this lesson off,\r\n",
        "\r\n",
        "5. Random search: example\r\n",
        "let's look at a full random search example. In lines 1-7, we load in the necessary modules, this time loading in RandomizedSearchCV from sklearn dot model_selection, and then load in and convert the data we need to a DMatrix object as always. In line 8 we create our parameter grid, this time generating a large number of learning rate values and subsample values using np-dot-arange. There are 20 values for learning_rate (or eta) and 20 values for subsample, which would be 400 models to try if we were to run a grid search (which we aren't doing here). In line 9 we create our xgbregressor object, and in line 10 we create our RandomizedSearchCV object, passing in the xgbregressor and parameter grid we had just created. We also set the number of iterations we want the random search to proceed to 25, so we know it will not be able to try all 400 possible parameter configurations. We also specify the evaluation metric we want to use, and that we want to run 4-fold cross-validation on each iteration. In line 11 we fit our randomizedsearchcv object, which can take a bit of time. Finally, lines 12 and 13 print the best model parameters found, and the corresponding best RMSE.\r\n",
        "\r\n",
        "6. Let's practice!\r\n",
        "Ok, now let's have you practice both grid search and random search in the following exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SlLJeKKaCeF"
      },
      "source": [
        "# Grid search with XGBoost\r\n",
        "\r\n",
        "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\r\n",
        "\r\n",
        "2. Instantiate an XGBRegressor object called gbm.\r\n",
        "\r\n",
        "3. Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. Also specify verbose=1 so you can better understand the output.\r\n",
        "\r\n",
        "4. Fit the GridSearchCV object to X and y.\r\n",
        "\r\n",
        "5. Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD9dV0nJsdZN"
      },
      "source": [
        "# Create the parameter grid: gbm_param_grid\r\n",
        "gbm_param_grid = {\r\n",
        "    'colsample_bytree': [0.3, 0.7],\r\n",
        "    'n_estimators': [50],\r\n",
        "    'max_depth': [2, 5]\r\n",
        "}\r\n",
        "\r\n",
        "# Instantiate the regressor: gbm\r\n",
        "gbm = xgb.XGBRegressor()\r\n",
        "\r\n",
        "# Perform grid search: grid_mse\r\n",
        "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring='neg_mean_squared_error', cv=4, verbose=1,)\r\n",
        "\r\n",
        "\r\n",
        "# Fit grid_mse to the data\r\n",
        "grid_mse.fit(X, y)\r\n",
        "\r\n",
        "# Print the best parameters and lowest RMSE\r\n",
        "print(\"Best parameters found: \", grid_mse.best_params_)\r\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Fitting 4 folds for each of 4 candidates, totalling 16 fits\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:31:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 50}\r\n",
        "    Lowest RMSE found:  29916.562522854438\r\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQh07Y3Osjyi"
      },
      "source": [
        "# Random search with XGBoost\r\n",
        "\r\n",
        "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\r\n",
        "\r\n",
        "Instructions\r\n",
        "\r\n",
        "1. Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\r\n",
        "\r\n",
        "2. Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\r\n",
        "\r\n",
        "3. Fit the RandomizedSearchCV object to X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M46DkhFtZ7K"
      },
      "source": [
        "# Create the parameter grid: gbm_param_grid \r\n",
        "gbm_param_grid = {\r\n",
        "    'n_estimators': [25],\r\n",
        "    'max_depth': range(2, 12)\r\n",
        "}\r\n",
        "\r\n",
        "# Instantiate the regressor: gbm\r\n",
        "gbm = xgb.XGBRegressor(n_estimators=10)\r\n",
        "\r\n",
        "# Perform random search: grid_mse\r\n",
        "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, scoring=\"neg_mean_squared_error\", n_iter=5, cv=4, verbose=1)\r\n",
        "\r\n",
        "\r\n",
        "# Fit randomized_mse to the data\r\n",
        "randomized_mse.fit(X, y)\r\n",
        "\r\n",
        "# Print the best parameters and lowest RMSE\r\n",
        "print(\"Best parameters found: \", randomized_mse.best_params_)\r\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))\r\n",
        "\r\n",
        "'''\r\n",
        "<script.py> output:\r\n",
        "    Fitting 4 folds for each of 5 candidates, totalling 20 fits\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:35:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    [06:36:01] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\r\n",
        "    Best parameters found:  {'n_estimators': 25, 'max_depth': 6}\r\n",
        "    Lowest RMSE found:  36909.98213965752\r\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz00OoZ5tuj2"
      },
      "source": [
        "# Limits of grid search and random search\r\n",
        "\r\n",
        "1. Limits of grid search and random search\r\n",
        "Now that you've done both GridSearch and RandomSearch for hyperparameter tuning on the Ames housing data, let's briefly go over the limits of both of these approaches for hyperparameter tuning.\r\n",
        "\r\n",
        "2. Grid search and random search limitations\r\n",
        "It should be clear to you that grid search and random search each suffer from distinct limitations. As long as the number of hyperparameters and distinct values per hyperparameter you search over is kept small, grid search will give you an answer in a reasonable amount of time. However, as the number of hyperparameters grows, the time it takes to complete a full grid search increases exponentially. For random search, the problem is a bit different. Since you can specify how many iterations a random search should be run, the time it takes to finish the random search wont explode as you add more and more hyperparameters to search through. The problem really is that as you add new hyperparameters to search over, the size of the hyperparameter space explodes as it did in the grid search case, and so you are left hoping that one of the random parameter configurations that the search chooses is a good one! You can always increase the number of iterations you want the random search to run, but then finding an optimal configuration becomes a combination of waiting randomly finding a good set of hyperparameters. In any case, both approaches have significant limitations.\r\n",
        "\r\n",
        "3. Let's practice!\r\n",
        "Great, now that you've learned how to tune the most important hyperparameters found in XGBoost, lets move onto the final chapter, where we work through 2 end to end processing pipelines utilizing XGBoost and scikit-learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTYyIbAzusyG"
      },
      "source": [
        "# When should you use grid search and random search?\r\n",
        "\r\n",
        "Now that you've seen some of the drawbacks of grid search and random search, which of the following most accurately describes why both random search and grid search are non-ideal search hyperparameter tuning strategies in all scenarios?\r\n",
        "\r\n",
        "Possible Answers\r\n",
        "\r\n",
        "1. Grid Search and Random Search both take a very long time to perform, regardless of the number of parameters you want to tune.\r\n",
        " - Sorry! This is not always the case.\r\n",
        "\r\n",
        "2. Grid Search and Random Search both scale exponentially in the number of hyperparameters you want to tune.\r\n",
        " - Nope! Only grid-search search time grows exponentially as you add parameters, random-search does not have to grow in this way.\r\n",
        "\r\n",
        "3. The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run.\r\n",
        " - This is why random search and grid search should not always be used. Nice!\r\n",
        " \r\n",
        "4. Grid Search and Random Search require that you have some idea of where the ideal values for hyperparameters reside.\r\n",
        "press\r\n",
        " - Although this is true, it is true of all hyperparameter search strategies, not just grid and random search."
      ]
    }
  ]
}